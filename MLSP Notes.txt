
Pandas Functions:
	1. loc - access rows and columns by labels
	2. iloc - access rows and columns by index.
	3. head - access first 4 rows.
	4. info - gives information about columns.
	5. df['column_name'].unique() - gives the unique column values.
	6. where - applies the filter condition(one or more) to the dataset and the rows not satisfying the condition is filled with NaN value
	7. value_counts - Return a Series containing counts of unique values.
			- normalize - give relative freq or normalize value of related to 1.
			- sort - sort by frequencies
			- ascending - sort in ascending order.
			- bins - creates bins and puts the count of values under these bins.
	8. corr - this method is used to find the pairwise correlation of all columns in the Pandas Dataframe in Python.
	Any NaN values are automatically excluded. Any non-numeric data type or columns in the Dataframe, it is ignored.
	While correlation is commonly used with numerical data, it is also possible to calculate the correlation between categorical variables.
	One common method for this is to use the chi-squared test of independence, which calculates a statistic that measures the degree
	of association between two categorical variables.
	9. reset_index - Reset the index of the DataFrame, and use the default one instead. If the DataFrame has a MultiIndex,
	this method can remove one or more levels.
	10. drop_duplicates - drop the duplicate values in the dataframe.
	11. pivot_table - create table with respect to the index and columns by showing various aggeregated values.
	
		Data : DataFrame, whose data is turned into pivot table.
		values : This is an optional parm. Column to aggregate.
		index : column, Grouper, array. Index is the feature that provides you to group the data.
		columns : column, Grouper, array, or list of the previous. Column, it is used for aggregating the values according to specific features.
		aggfunc : It is an aggregation function and we can set this param with a list of functions, dict, default is numpy.mean. 
		If it is set to a list of functions, the resulting pivot table forms a hierarchical column and this list of functions will be a top-level column.
		If it is set to dictionary the key is a column to aggregate and the value is a function or list of functions.
		Aggregate function aggfunc param takes np.mean() function as a default function for grouping the data. 
		
	12. drop - In order to remove columns use axis=1 or columns param. For example df.drop("Discount",axis=1) 
		removes Discount column by kepping all other columns untouched. This gives you a DataFrame with all columns with out one unwanted column.
		
	13. difference -  Using Series.difference() Method to Select All Columns Except One
	You can use list of columns you don’t wante to select to Series.difference() Method. For E.x, df[df.columns.difference(["Fee"])] select all columns,
	except one “Fee” column in Dataframe. or You can also try using isin() method with negate operator for example:
	df.loc[:, ~df.columns.isin(['Fee'])] This returns a DataFrame with all columsn except Fee column.
		
	
	
	
Pandas attributes:
	1. columns - gives the columns of dataset.
	2. values - gives the values of the dataset.
	3. dtypes -  gives the columns datatypes of the dataframe.
	4. 


difference between loc and iloc:

loc -  This function is primarily label based, but it’s also used with a boolean array when we create statements.
If we want to look at only rows for male customers here, we would use pandas. DataFrame.loc to locate with labels.
pandas.DataFrame.loc is used for accessing multiple columns.

titanic.loc[titanic[‘Sex’]==’male’]

iloc:

The .iloc function is integer position based, but it could also be used with a boolean array. If we want to locate a cell of the data set, we can enter.

titanic.iloc[0,0]

This command gives the element at row = 0, and column = 0. I can also extract a slice of the data set.

titanic.iloc[0:4,2:5]

it provides us rows zero-to-three and columns two-to-four.Function .loc is primarily used for label indexing, 
and the .iloc function is mainly used for integer indexing.

Pandas GroupBy and Transform Function:

	In this article, we will cover the following most frequently used Pandas transform() features:

		1. Transforming values
		2. Combining groupby() results
		3. Filtering data
		4. Handling missing value at the group level
		
Let’s take a look at pd.transform(func, axis=0)

	The first argument func is to specify the function to be used for manipulating data. It can be a function, a string function name,
	a list of functions, or a dictionary of axis labels -> functions
	
	func can be a list of functions. for example sqrt and exp from NumPy.
	
	A dict of axis labels -> function. func can be a dict of axis labels -> function. For example
	
	The 2nd argument axis is to specify which axis the func is applied to. 0 for applying the func to each column and 1 for applying 
	the func to each row. Let’s see how transform() works with the help of some examples.
	
	df = pd.DataFrame({'col_A': [1,2,3], 'col_B': [10,20,30] })
	def plus_10(x):
		return x+10
	df.transform(plus_10)
	
	index col_A	  col_B

	  0    11	    20

	  1    12		30
	  
	  2    13		40
	  
2. Combining groupby() results
		
		One of the most compelling usages of Pandas transform() is combining grouby() results.
		
		
There are 2 solutions:

Solution 1: groupby(), apply(), and merge()

	The first solution is splitting the data with groupby() and using apply() to aggregate each group, then merge the results back
into the original DataFrame using merge().By default, the merge function performs an inner join.

Using merge function to combine the results:
	
	city_sales = df.groupby('city')['sales'].sum().rename('city_total_sales').reset_index()
	df_new = pd.merge(df, city_sales, how='left')
	
	The group results get merged back into the original DataFrame using merge() with how='left' for left outer join.
	
Inner Join:

	Inner join is the most common type of join you’ll be working with. It returns a dataframe with only those 
rows that have common characteristics.

	An inner join requires each row in the two joined dataframes to have matching column values. 
This is similar to the intersection of two sets.

The return value type of the merge function is dataframe.

There is another function in pandas called the concat function. From the name, it suggests that we can concatenate strings
using this function, but it actually just concatenates two dataframes.

Here, I have performed inner join on the product and customer dataframes on the ‘Product_ID’ column.

But what if the column name strings are different in the two dataframes? Then, we have to explicitly mention both column names str.
we cannot pass empty strings in left_on or right_on arguments.

We can pass an iterable object of column names to the left_on and right_on arguments, 
which can be a list or a tuple, as they are iterable in left_on or right_on:

‘left_on’ and ‘right_on’ are two parameters through which we can achieve this. ‘left_on’ is the name of the key in the left dataframe 
and ‘right_on’ in the right dataframe, so the syntax will be:

pd.merge(product,customer,left_on='Product_name',right_on='Purchased_Product')

Full Join in Pandas:

	We have to combine both dataframes in order to find all the products that are not sold and all the customers who didn’t
	purchase anything from us.

	Full Join, also known as Full Outer Join, returns all those records which either have a match in the left or right dataframe.
	
	When rows in both the dataframes do not match, the resulting dataframe will have NaN for every column of the dataframe that 
	lacks a matching row.

	We can perform Full join by passing the how parameter as ‘outer’ to the merge() function, which takes input python strings as arguments:

	pd.merge(product,customer,on='Product_ID',how='outer')
	
	By mentioning the indicator argument as True in the function, a new column of name _merge will be created in the resulting dataframe.
	
Left Join in Pandas:
	
	Now, let’s say the leadership team wants information about only those customers who bought something from us.
	
	Left join, also known as Left Outer Join, returns a dataframe containing all the rows of the left dataframe.
	
	All the non-matching rows of the left dataframe contain NaN for the columns in the right dataframe. 
	It is simply an inner join plus all the non-matching rows of the left dataframe filled with NaN for columns 
	of the right dataframe.
	
	Performing a left join is actually quite similar to a full join. Just change 
	the how argument to a different str that is ‘left’:


Right Join in Pandas:

Similarly, if we want to create a table of customers including the information about the products they bought, we can use the right join.

	Right join, also known as Right Outer Join, is similar to the Left Outer Join.
	The only difference is that all the rows of the right dataframe are taken as it is and only those of the left dataframe
	that are common in both.
	
	Similar to other joins, we can perform a right join by changing the how argument to ‘right’
	
Handling Redundancy/Duplicates in Python Joins:

	Duplicate values can be tricky obstacles. They can cause problems while performing joins.
	These values won’t give an error but will create redundancy in our resulting dataframe.

	As you can see, we have duplicate rows in the resulting dataset as well. To solve this, there is a validate argument in the merge() function,
	which we can set to ‘one_to_one,’ ‘one_to_many,’ ‘many_to_one,’ and‘many_to_many.’

	This ensures that there exists only a particular mapping across both the dataframes. If the mapping condition is not satisfied,
	then it throws a MergeError.
	
	pd.merge(product_dup.drop_duplicates(),customer,how='inner',on='Product_ID')

	But, if you want to keep these duplicates, then you can give validate values as per your requirements, and it will not throw an error:

	pd.merge(product_dup,customer,how='inner',on='Product_ID',validate='many_to_many')
	
Pandas Index Join
	
	To merge the Dataframe on indices pass the left_index and right_index arguments as True i.e. both the Dataframes are merged on an 
	index using default Inner Join.
	
import pandas as pd
 
	# Creating dataframe a
	a = pd.DataFrame()
	 
	# Creating Dictionary
	d = {'id': [1, 2, 10, 12],
		 'val': ['a', 'b', 'c', 'd']}
	 
	a = pd.DataFrame(d)
	 
	# Creating dataframe b
	b = pd.DataFrame()
	 
	# Creating dictionary
	d = {'id': [1, 2, 9, 8],
		 'val': ['p', 'q', 'r', 's']}
	b = pd.DataFrame(d)
	 
	# index join
	df = pd.merge(a, b, left_index=True, right_index=True)
	 
	# display dataframe
	df
	
	id_x val_x id_y val_y
 0	  1    a     1    p
	  
 1	  2    b     2    q
	  
 2	  10   c     9    r
	  
 3	  12   d     8    s
	
	
groupby() and transform()

	Method 1: Use groupby() and transform() with built-in function
	
		df['new'] = df.groupby('group_var')['value_var'].transform('mean')
	
	Method 2: Use groupby() and transform() with custom function
	
		df['new'] = df.groupby('group_var')['value_var'].transform(lambda x: some function)
		
The most important thing about the Pandas GroupBy Transform is that it can only be applied to a single column at once and 
can not be applied to multiple columns at once. If you want to access multiple columns at once and do computation on it you can pre-compute 
the DataFrame and then apply the transform().


Label Encoding vs Ordinal Encoding vs One Hot Encoding:

Label encoding is used when there is no intrinsic order or hierarchy among the categories in a feature.One Hot Encoding is used when the categorical feature
has no intrinsic order or hierarchy among its categories and when the categories are not related to each other in any meaningful way.

Ordinal encoding is used when there is an intrinsic order or hierarchy among the categories in a feature.



Regex Patterns:
\d is a digit (a character in the range [0-9]), and + means one or more times. Thus, \d+ means match one or more digits.

\s stands for “whitespace character”. Again, which characters this actually includes, depends on the regex flavor. 
In all flavors discussed in this tutorial, it includes [ \t\r\n\f]. That is: \s matches a space, a tab, a carriage return, a line feed

Placing r or R before a string literal creates what is known as a raw-string literal. 
Raw strings do not process escape sequences (\n, \b, etc.) and are thus commonly used for Regex patterns, which often contain a lot of \ characters.


\ means	Signals a special sequence (can also be used to escape special characters)
. means "any character".
* means "any number of this".
.* therefore means an arbitrary string of arbitrary length.
^ indicates the beginning of the string.
+ indicates one or more occurrences.
$ indicates the end of the string.
? indicates	Zero or one occurrences.
| indicates	Either or.
{} indicates Exactly the specified number of occurrences.

txt = "hello planet"

we used re.search() to find the first match for a pattern. 
findall() finds *all* the matches and returns them as a list of strings, with each string representing one match.

#Search for a sequence that starts with "he", followed excactly 2 (any) characters, and an "o":

x = re.findall("he.{2}o", txt)

print(x)
['hello']

The regular expression says: There may be any number of characters between the expression (?=.{8,})(?=.*[a-z])(?=.*[A-Z])(?=.*[@#$%^&+=]) 
and the beginning and end of the string that is searched.

^.* //Start of string followed by zero or more of any character (except line break)

.*$ //Zero or more of any character (except line break) followed by end of string


Tensor.dim() → int
Returns the number of dimensions of self tensor.


Re regex Module:

https://www.w3schools.com/python/python_regex.asp


The re module offers a set of functions that allows us to search a string for a match:

Function		Description
findall		Returns a list containing all matches
search		Returns a Match object if there is a match anywhere in the string
split		Returns a list where the string has been split at each match
sub			Replaces one or many matches with a string
	
----------------------------------------------------------------------------------------------------------------------------------------	
SNS PairPlot:
	
	To plot multiple pairwise bivariate distributions in a dataset, you can use the .pairplot() function.

SNS BoxPlot:

Draw a box plot to show distributions with respect to categories.

	A box plot (or box-and-whisker plot) shows the distribution of quantitative data in a way that facilitates comparisons between variables or across levels of a 
categorical variable. The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution, except for points that are 
determined to be “outliers” using a method that is a function of the inter-quartile range.

SNS CountPlot:
	Show the counts of observations in each categorical bin using bars.

A count plot can be thought of as a histogram across a categorical, instead of quantitative, variable. Show the number of datapoints with each value of a
categorical variable.

SNS KdePlot:
		a Kernel Distribution Estimation Plot which depicts the probability density function 
of the continuous or non-parametric data variables i.e. we can plot for the univariate or multiple
variables altogether.Kernel Density Estimation plot for the seaborn library.	
	
OrdinalEncoder:


LabelEncoder:	
	A bit difference is the idea behind. OrdinalEncoder is for converting features, while LabelEncoder is for converting target variable.

That's why OrdinalEncoder can fit data that has the shape of (n_samples, n_features) while LabelEncoder can only fit data that has the shape of (n_samples,) 
(though in the past one used LabelEncoder within the loop to handle what has been becoming the job of OrdinalEncoder now).
	
OrdinalEncoder is for 2D data with the shape (n_samples, n_features)
LabelEncoder is for 1D data with the shape (n_samples,)

What is the difference between LabelEncoder and Ordinalencoder?
LabelEncoder should be used to encode target values, i.e. y, and not the input X. Ordinal encoding should be used for ordinal variables
(where order matters, like cold , warm , hot ); 
vs Label encoding should be used for non-ordinal (aka nominal) variables (where order doesn't matter, like blonde , brunette )


S.No.	OrdinalEncoder											LabelEncoder										OneHotEncoder
1		Used to encode the input.							Used to encode the output 							Used to encode the input.
															(aka target or label).								
2		Returns a single column of integers
		(0 to n_categories - 1) per feature.				Returns a single column of integers              Returns a single column (1 indicating presence
																(0 to n_labels - 1).						of a feature and 0 indicating absence) per feature.
																											
ValueError : Truth value of a series is ambiguous Use a.empty() a.bool() or a.item() or a.any() or a.all()

https://towardsdatascience.com/fix-valueerror-truth-value-of-a-series-is-ambiguous-pandas-a683f0fd1b2f


numpy einsum:

https://stackoverflow.com/questions/26089893/understanding-numpys-einsum

What does einsum do?
Imagine that we have two multi-dimensional arrays, A and B. Now let's suppose we want to...

multiply A with B in a particular way to create new array of products; and then maybe
sum this new array along particular axes; and then maybe
transpose the axes of the new array in a particular order.
There's a good chance that einsum will help us do this faster and more memory-efficiently than combinations of the NumPy 
functions like multiply, sum and transpose will allow.


numpy.apply_along_axis(func1d, axis, arr, *args, **kwargs)[source]
	
		Apply a function to 1-D slices along the given axis.

axis -  0 represents the columns and 1 represents the rows of the matrix.


numpy.apply_over_axes(func, a, axes)[source]
	
	The numpy.apply_over_axes()applies a function repeatedly over multiple axes in an array.

Syntax : 

	numpy.apply_over_axes(func, array, axes)
	
	1d_func  : the required function to perform over 1D array. It can only be applied in 
1D slices of input array and that too along a particular axis. 
	axis     : required axis along which we want input array to be sliced
	array    : Input array to work on


---------------------------------------------------------------------------------------------------------------------------------

Statistical Tests:

Quantitative variables are any variables where the data represent amounts.

Categorical variables are any variables where the data represent groups.


Left Skewed:

Left skewed distributions occur when the long tail is on the left side of the distribution. Statisticians also refer to them as negatively skewed.

In box plot, When the median is closer to the box’s higher values and the lower whisker is longer, it’s a left skewed distribution.

Right Skewed:

Right skewed distributions occur when the long tail is on the right side of the distribution. Analysts also refer to them as positively skewed.

In box plot, When the median is closer to the box’s lower values and the upper whisker is longer, it’s a right skewed distribution.
Notice how the longer tail extends into the higher values, making it positively skewed.

The mean, median, and mode are all equal in the normal distribution and other symmetric distributions.

Right skewed: The mean is greater than the median. The mean overestimates the most common values in a positively skewed distribution.

Left skewed: The mean is less than the median. The mean underestimates the most common values in a negatively skewed distribution.

Because the mean over or underestimates the most frequently occurring values in asymmetric distributions, analysts often use the median in these cases.


To remove skewness 3 most common methods used are:
1. Log Transform
2. Square Root Transform
3. Box Cox Transform.


A correlation matrix serves as a diagnostic for regression.

One key assumption of multiple linear regression is that no independent variable in the model is highly correlated with
another variable in the model.

When two independent variables are highly correlated, this results in a problem known as multicollinearity and
it can make it hard to interpret the results of the regression.

One of the easiest ways to detect a potential multicollinearity problem is to look at a correlation matrix and visually check 
whether any of the variables are highly correlated with each other.


Null & Alternate Hypothesis:

A p-value is a statistical measurement used to validate a hypothesis against observed data.
A p-value measures the probability of obtaining the observed results, assuming that the null hypothesis is true.
The lower the p-value, the greater the statistical significance of the observed difference.
A p-value of 0.05 or lower is generally considered statistically significant.

Null: Two sample means are equal.

Alternate: Two sample means are not equal

To reject a null hypothesis, one needs to calculate test statistics, then compare the result
with the critical value. If the test statistic is greater than the critical value, we can reject the null hypothesis.


p-value ≤  α
 : significant result, reject null hypothesis.
p-value >  α
 : not significant result, fail to reject the null hypothesis.
 
 
Null hypothesis (H0): There is no significant association between the two variables.

Alternative hypothesis: (Ha): There is a significant association between the two variables.

STATISTICAL TESTS: THE T-TEST, THE CHI-SQUARE AND MORE
Z-Test
T-Test
Chi-Square Test
ANOVA

T-Test:

We use a t-test to compare the mean of two given samples. Like a z-test, a t-test also assumes a normal 
distribution of the sample. When we don’t know the population parameters (mean and standard deviation), we use t-test.

THE THREE VERSIONS OF A T-TEST

Independent sample t-test: compares mean for two groups
Paired sample t-test: compares means from the same group at different times
One sample t-test: tests the mean of a single group against a known mean


The statistic for this hypothesis testing is called t-statistic, the score for which we calculate as:

		t=(x1—x2) / (σ / √n1 + σ / √n2), where

x1=mean of sample 1

x2=mean of sample 2

n1=sample size 1

n2=sample size 2

There are multiple variations of the t-test.

This article focuses on normally distributed data. You can use z-tests and t-tests for data which is non-normally distributed
as well if the sample size is greater than 20, however there are other preferable methods to use in such a situation.


Chi-Square Test:

We use the chi-square test to compare categorical variables.

When to use the chi-square test of independence

There’s another type of chi-square test, called the chi-square test of independence.

Use the chi-square goodness of fit test when you have one categorical variable and you want to test a hypothesis about its distribution.
Use the chi-square test of independence when you have two categorical variables and you want to test a hypothesis about their relationship.

THE TWO TYPES OF CHI-SQUARE TEST

Goodness of fit test: determines if a sample matches the population

A chi-square fit test for two independent variables: used to compare two variables in a contingency table to check if the data fits
A small chi-square value means that data fits.

A large chi-square value means that data doesn’t fit.

The hypothesis we’re testing is:

Null: Variable A and Variable B are independent.
Alternate: Variable A and Variable B are not independent.

The statistic used to measure significance, in this case, is called chi-square statistic. The formula we use to calculate the statistic is:

Χ2 = Σ [ (Or,c — Er,c)2 / Er,c ] where

Or,c=observed frequency count at level r of Variable A and level c of Variable B.

Er,c=expected frequency count at level r of Variable A and level c of Variable B.


You have seen the  χ2
  test statistic used in three different circumstances. The following bulleted list is a summary that will help you decide which  χ2
  test is the appropriate one to use.

Goodness-of-Fit: Use the goodness-of-fit test to decide whether a population with an unknown distribution "fits" a known distribution. In this case 
there will be a single qualitative survey question or a single outcome of an experiment from a single population. Goodness-of-Fit is typically used 
to see if the population is uniform (all outcomes occur with equal frequency), the population is normal, or the population is the same as another 
population with a known distribution. The null and alternative hypotheses are:
	
	H0: The population fits the given distribution.
	Ha: The population does not fit the given distribution.

Independence: Use the test for independence to decide whether two variables (factors) are independent or dependent. In this case there will be
two qualitative survey questions or experiments and a contingency table will be constructed. The goal is to see if the two variables are unrelated 
(independent) or related (dependent). The null and alternative hypotheses are:
	
	H0: The two variables (factors) are independent.
	Ha: The two variables (factors) are dependent.

Homogeneity: Use the test for homogeneity to decide if two populations with unknown distributions have the same distribution as each other.
In this case there will be a single qualitative survey question or experiment given to two different populations. The null and alternative hypotheses
are:

	H0: The two populations follow the same distribution.
	Ha: The two populations have different distributions.


Monotonic Relation: 

A monotonic relationship is a relationship that does one of the following: 
(1) as the value of one variable increases, so does the value of the other variable;
(2) as the value of one variable increases, the other variable value decreases.



ANOVA

We use analysis of variance (ANOVA) to compare three or more samples with a single test. 

THE TWO MAJOR TYPES OF ANOVA

One-way ANOVA: Used to compare the difference between three or more samples/groups of a single independent variable.

MANOVA: Allows us to test the effect of one or more independent variables on two or more dependent variables. 
In addition, MANOVA can also detect the difference in correlation between dependent variables given the groups of independent variables.

The statistics used to measure the significance in this case are F-statistics. We calculate the F-value using the formula:

		F= ((SSE1—SSE2)/m)/ SSE2/n-k,

where

	SSE=residual sum of squares

	m=number of restrictions

	k=number of independent variables

There are multiple tools available such as SPSS, R packages, Excel etc.
to carry out ANOVA on a given sample.

In all of these tests we’re comparing a statistic with a critical value to accept or reject a hypothesis. However, the statistic and the way 
to calculate it differ depending on the type of variable, the number of samples you’re analyzing and whether or not we know the population parameters.
We can thus choose a suitable statistical test and null hypothesis.

Fisher’s exact test

Fisher’s exact test is an alternative to Pearson’s chi-squared test for independence. While actually valid for all sample sizes,
Fisher’s exact test is practically applied when sample sizes are small. A general recommendation is to use Fisher’s exact test- 
instead of the chi-squared test - whenever more than 20 % of cells in a contingency table have expected frequencies < 5.

Fisher’s exact test is used to determine whether there is a significant association between two categorical variables in a contingency table.

What is the difference between a chi-square test and a t test?

Chi-Square Test for independence: Allows you to test whether or not not there is a statistically significant association between 
two categorical variables. When you reject the null hypothesis of a chi-square test for independence, it means there is a significant association
between the two variables.

Here's a step-by-step guide to interpreting the p-value in a chi-square test:

	1. Set up the null hypothesis (H0) and the alternative hypothesis (H1).
	2. Compute the chi-square statistic using the observed data and expected frequencies.
	3. Determine the degrees of freedom (df) for the chi-square test. This depends on the number of categories or groups being compared.
	4. Calculate the p-value associated with the chi-square statistic using the chi-square distribution table or statistical software.
	5. Compare the p-value to the chosen significance level (α).
			If p-value ≤ α, reject the null hypothesis (H0).
			If p-value > α, fail to reject the null hypothesis (H0).

When variables are dependent, it means that there is a relationship or association between them. In the context of statistical analysis, dependency between variables can be examined using various
statistical tests and measures.

If you have two categorical variables, you can determine their dependency using a chi-square test of independence. The chi-square test compares the observed frequencies of the categories with the expected
frequencies under the assumption that the variables are independent. If the test results in a low p-value (typically below the chosen significance level), it suggests that the variables are dependent.

When variables are dependent, it implies that changes in one variable are associated with changes in the other variable. The strength and nature of the dependency can further be explored using measures like Cramér's V,
contingency coefficients, or phi coefficient, which provide information about the magnitude of the relationship.

In the case of continuous variables, the dependency can be assessed using correlation coefficients such as Pearson's correlation coefficient or Spearman's rank correlation coefficient. These coefficients measure the 
strength and direction of the linear relationship between the variables.

If variables are dependent, there are several types of modeling techniques that can be used to explore and understand the relationship between them.

1. Linear Regression
2. Logistic Regression.
3. Decision Trees
4. Time Series Analysis.



t-Test for a difference in means: Allows you to test whether or not there is a statistically significant difference between two population means.
When you reject the null hypothesis of a t-test for a difference in means, it means the two population means are not equal.

Both chi-square tests and t tests can test for differences between two groups. However, a t test is used when you have a dependent quantitative 
variable and an independent categorical variable (with two groups). A chi-square test of independence is used when you have 
two categorical variables.

If you have two variables that are both categorical, i.e. they can be placed in categories like male, female and republican,
democrat, independent, then you should use a chi-square test.

But if one variable is categorical (e.g. type of study plan – either plan 1 or plan 2) and the other is continuous (e.g. exam score – 
measured from 0 to 100), then you should use a t-test.

-------------------------------------------------------------------------------------------------------------------------------

The output array. Shape of the output array can be different depending on whether func 
changes the shape of its output with respect to its input.

https://www.geeksforgeeks.org/numpy-apply_over_axes-python/


Statistical Tests:

https://www.kaggle.com/code/gauravsharma99/statistical-analysis-on-mpg-data

Data Visualization:

https://towardsdatascience.com/data-visualization-using-matplotlib-16f1aae5ce70


------------------------------------------------------------------------------------------------------------------------------
Python Function Arguments:

	A function in python can have unknown number of arguments by putting * in front of the parameter passed to the function.
	
Function with keyword Arguments: 
	 
	In order to call a function with arguments, the same number of actual arguments must be called. However the arguments can
be passed in any order while calling the function.	

# sum_integers_args_2.py
def my_sum(*integers):
    result = 0
    for x in integers:
        result += x
    return result

print(my_sum(1, 2, 3))

The function still works, even if you pass the iterable object as integers instead of args. All that matters here is that you 
use the unpacking operator (*). The arguments passed is tuples so that they are immutable.

Keyword Argument (**kwargs):
	
	The function have a single parameter prefixed with **. This type of parameter initialized to a new ordered mapping recieving
any excess keyword arguments, defaulting to a new empty mapping of the same type. **kwargs works just like *args,
but instead of accepting positional arguments it accepts keyword (or named) arguments. Take the following example:

def greet(**person):
	print('Hello ', person['firstname'],  person['lastname'])

greet(firstname='Steve', lastname='Jobs')
greet(lastname='Jobs', firstname='Steve')
greet(firstname='Bill', lastname='Gates', age=55) 
greet(firstname='Bill') # raises KeyError 


Difference between Dataset and TensorDataset in Pytorch:

https://stackoverflow.com/questions/67683406/difference-between-dataset-and-tensordataset-in-pytorch

If you just want to create a dataset that contains tensors for input features and labels, then use the TensorDataset directly:

	dataset = TensorDataset(input_features, labels)

Note that input_features and labels must match on the length of the first dimension.


--------------------------------------------------------------------------------------------------------------------------------

The numpy.ravel() functions returns contiguous flattened array(1D array with all the input-array elements and with the
same type as it). A copy is made only if needed.

Return:
	Flattened array having same type as the Input array and and order as per choice. 
	
	
array.ravel is equivalent to reshape(-1, order=order)

--------------------------------------------------------------------------------------------------------------------------------
Python Dictionary Comprehension:

Dictionary comprehension is an elegant and concise way to create dictionaries.Dictionaries are data types in Python which allows 
us to store data in key/value pair.
	
Syntax:
	The minimal syntax for dictionary comprehension is:
		dictionary = {key: value for vars in iterable}

Conditionals in Dictionary Comprehension
	We can further customize dictionary comprehension by adding conditions to it. Let's look at an example.

Eg:

original_dict = {'jack': 38, 'michael': 48, 'guido': 57, 'john': 33}

even_dict = {k: v for (k, v) in original_dict.items() if v % 2 == 0}
print(even_dict)
{'jack': 38, 'michael': 48}
	

Eg:
# dictionary comprehension example
square_dict = {num: num*num for num in range(1, 11)}
print(square_dict)

The output of both programs will be the same.
{1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81, 10: 100}


Python Dictionary Functions:

update - the update() method updates the dictionary with the elements from another dictionary object or from an iterable of 
key/Value pairs

update() method updates the dictionary with elements from a dictionary object or an iterable object of key/value pairs.

It doesn't return any value (returns None).

Eg:
	d = {1: "one", 2: "three"}
	d1 = {2: "two"}

	# updates the value of key 2
	  d.update(d1)
 
    print(d)
	{1: 'one', 2: 'two'}

	dictionary = {'x': 2}

	dictionary.update([('y', 3), ('z', 0)])

	print(dictionary)
	{'x': 2, 'y': 3, 'z': 0}
	
keys - The keys() method extracts the keys of the dictionary and returns the list of keys as a view object. update in the dictionary
updates the view object.

the keys method returns:
	1. a view object that returns the list of all keys.

numbers = {1: 'one', 2: 'two', 3: 'three'}

# extracts the keys of the dictionary
dictionaryKeys = numbers.keys()

print(dictionaryKeys)
O/P: dict_keys([1, 2, 3])

get - The get method returns the value for the specified key if the key is in the dictionary.
 
Python get() method Vs dict[key] to Access Elements - get() method returns a default value if the key is missing.
However, if the key is not found when you use dict[key], KeyError exception is raised.

Syntax:
dict.get(key[, value])

Parameters:
get() method takes maximum of two parameters:
	key - key to be searched in the dictionary.
	value - value to be returned if key is not found. the default value is None.

Returns:
get() method returns:

	the value -  for the specified key if key is in the dictionary.
	None -  if the key is not found and value is not specified.
	value  - if the key is not found and value is specified.

Eg:
1. marks = {'Physics':67, 'Maths':87}

print(marks.get('Physics'))

O/P: 67

2. a = {'one':1 , 'two':2, 'three':3}

print(a.get('four',0.0))

O/P: 0.0 # default value is returned.

fromkeys - The fromkeys() method creates a dictionary from the given sequence of keys and values.

Parameters:

The fromkeys() method can take two parameters:
	alphabets - are the keys that can be any iterables like string, set, list, etc.
	numbers (Optional) - are the values that can be of any type or any iterables like string, set, list, etc.

	Note: The same value is assigned to all the keys of the dictionary.
	
Return:

	The fromkeys() method returns:  a new dictionary with the given sequence of keys and values
	
	Note: If the value of the dictionary is not provided, None is assigned to the keys.

Eg:
1. 
# keys for the dictionary
 alphabets = {'a', 'b', 'c'}

# value for the dictionary
number = 1

# creates a dictionary with keys and values
dictionary = dict.fromkeys(alphabets, number)

print(dictionary)
O/P: {'a': 1, 'c': 1, 'b': 1}

2.
# list of numbers
keys = [1, 2, 4 ]

# creates a dictionary with keys only
numbers = dict.fromkeys(keys)

print(numbers)
O/P: {1: None, 2: None, 4: None} # Default None value is assigned.

Syntax:
The syntax of the fromkeys() method is:
dict.fromkeys(alphabets,number)
Here, alphabets and numbers are the key and value of the dictionary.

--------------------------------------------------------------------------------------------------------------------------------

Python Globals and Locals Function:

	A symbol table is a data structure maintained by a compiler which contains all necessary information
about the program. These include variable names,methods,classes etc;
	
There are two kinds of symbol table:
		1. Local Symbol Table.
		2. Global Symbol Table.

Local Symbol Table stores all information related to the local scope of the program, and is accessed in
Python using locals() method. The Local Scope could be within a function,class etc;

Likewise, a Global Symbol Table stores all information related to the global scope of the program, and is
accessed in Python using globals() method. The Global Scope contains all functions and variables that are
not associated with any class or function.

The globals table dictionary is the dictionary of the current module(inside a function, that is a module
where it is defined, not where the model is called). globals() method doesn't take any parameters.


--------------------------------------------------------------------------------------------------------------------------------
Pytorch Functions:	

torch.jit.script

	If obj is nn.Module, script returns a ScriptModule object. The returned ScriptModule will have the same set of sub-modules and 
parameters as the original nn.Module. If obj is a standalone function, a ScriptFunction will be returned. If obj is a dict, then script 
returns an instance of torch._C.ScriptDict. If obj is a list, then script returns an instance of torch._C.ScriptList.
	
	Scripting an nn.Module by default will compile the forward method and recursively compile any methods, submodules, and functions 
called by forward. If a nn.Module only uses features supported in TorchScript, no changes to the original module code should be necessary.
script will construct ScriptModule that has copies of the attributes, parameters, and methods of the original module.

To compile a method other than forward (and recursively compile anything it calls), add the @torch.jit.export decorator to the method.
To opt out of compilation use @torch.jit.ignore or @torch.jit.unused.


What is the difference between Tensor.size and Tensor.shape, Tensor.numel() in Pytorch?

Tensor.size and Tensor.shape, Tensor.numel() -  these are metadata for a tensor object.

torch.cumsum(input, dim, *, dtype=None, out=None) - 

Returns the cumulative sum of elements of input in the dimension dim.

For example, if input is a vector of size N, the result will also be a vector of size N, with elements.

	yi = x1 + x2 + x3 + x4 + x5 + x6 + x7 + ... + xi.
	


Tensor.Size - this is a function that is same as shape attribute.
Tensor.Shape - this is  a attribute that is same as size function.
Tensor.numel - this gives the total number of elements present in the tensor object.
------------------------------------------------------------------------------------------------------------------------------------
What does x.numel() do for a tensor x

numel() method returns the total number of elements in the input tensor.
Syntax: torch.numel(input)
Arguments.
Return: It returns the length of the input tensor.

What does x.contiguous() do for a tensor x

There are a few operations on Tensors in PyTorch that do not change the contents of a tensor, but change the way the data is 
organized. These operations include:

narrow(), view(), expand() and transpose()

For example: when you call transpose(), PyTorch doesn't generate a new tensor with a new layout, it just modifies meta 
information in the Tensor object so that the offset and stride describe the desired new shape. In this example, the transposed 
tensor and original tensor share the same memory:

x = torch.randn(3,2)
y = torch.transpose(x, 0, 1)
x[0, 0] = 42
print(y[0,0])
# prints 42

This is where the concept of contiguous comes in. In the example above, x is contiguous but y is not because its memory 
layout is different to that of a tensor of same shape made from scratch. Note that the word "contiguous" is a bit misleading 
because it's not that the content of the tensor is spread out around disconnected blocks of memory. Here bytes are still 
allocated in one block of memory but the order of the elements is different!

When you call contiguous(), it actually makes a copy of the tensor such that the order of its elements in memory is the same 
as if it had been created from scratch with the same data.

Normally you don't need to worry about this. You're generally safe to assume everything will work, and wait until you get a 
RuntimeError: input is not contiguous where PyTorch expects a contiguous tensor to add a call to contiguous().

---------------------------------------------------------------
	torch.nn.Parameter(torch.FloatTensor(2,2))
	
I will break it down for you. Tensors, as you might know, are multi dimensional matrices. Parameter, in its raw form, is a 
tensor i.e. a multi dimensional matrix. It sub-classes the Variable class.

The difference between a Variable and a Parameter comes in when associated with a module. When a Parameter is associated with 
a module as a model attribute, it gets added to the parameter list automatically and can be accessed using the 'parameters' 
iterator.

Initially in Torch, a Variable (which could for example be an intermediate state) would also get added as a parameter of the 
model upon assignment. Later on there were use cases identified where a need to cache the variables instead of having them added 
to the parameter list was identified.

One such case, as mentioned in the documentation is that of RNN, where in you need to save the last hidden state so you don't 
have to pass it again and again.The need to cache a Variable instead of having it automatically register as a parameter to the
model is why we have an explicit way of registering parameters to our model i.e. nn.Parameter class.

In the online official guide, it says ‘torch.Tensor() is just an alias to torch.FloatTensor()’. 
And from the torch for numpy users notes, it seems that torch.FloatTensor() is a drop-in replacement 
of numpy.empty() 
---------------------------------------------------------------
torch.scatter(dim, index, src)

The official document scatter_(dim, index, src) → Tensor tells us that parameters include the dim, index tensor, and the source
tensor. dim specifies where the index tensor is functioning, and we will keep the other dimensions unchanged. And as the 
function name suggests, the goal is to scatter values in the source tensor to the input tensor self. What we are going to do 
is to loop through the values in the source tensor, find its position in the input tensor, and replace the old one.

https://yuyangyy.medium.com/understand-torch-scatter-b0fd6275331c

---------------------------------------------------------------

	torch.full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
	
Creates a tensor of size size filled with fill_value. The tensor’s dtype is inferred from fill_value.

Parameters:
	size (int...) – a list, tuple, or torch.Size of integers defining the shape of the output tensor.

	fill_value (Scalar) – the value to fill the output tensor with.
	
Keyword Arguments:
	out (Tensor, optional) – the output tensor.

	dtype (torch.dtype, optional) – the desired data type of returned tensor. Default: if None, uses a global default 
(see torch.set_default_tensor_type()).

	layout (torch.layout, optional) – the desired layout of returned Tensor. Default: torch.strided.

	device (torch.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the
default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA
 device for CUDA tensor types.

requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.

Example:

	torch.full((2, 3), 3.141592)
	
	tensor([[ 3.1416,  3.1416,  3.1416],
        [ 3.1416,  3.1416,  3.1416]]
		)

---------------------------------------------------------------
	torch.nn.Linear(in_features, out_features, bias=True)

We can set bias to False to make nn.Linear() perform like a simple matrix transformation. 
In my case, I used

	weight = torch.nn.Linear(2, 2, bias=False)

---------------------------------------------------------------
nn.Embedding() creates a simple lookup table that stores embeddings of a fixed dictionary and size. 
This module is often used to store word embeddings and retrieve them using indices. The input to 
the module is a list of indices, and the output is the corresponding word embeddings.

To create a nn.Embedding() parameter, a typical instance is

	nn.Embedding(num_embedding, embedding_dim)

nn.Parameter() receives the tensor that is passed into it, and does not do any initial processing such as
 uniformization. That means that if the tensor passed into is empty or uninitialized, the parameter will
also be empty or uninitialized. But nn.Linear() and nn.Embedding() initialize their weight tensors with 
the uniform operation and normalization operation respectively. You won’t get an empty parameter even 
you only give the shape.

--------------------------------------------------------------
	torch.sum(input, dim, keepdim=False, dtype=None) → Tensor
	
	input (Tensor) – the input tensor.
	dim (int or tuple of python:ints) – the dimension or dimensions to reduce.
	keepdim (bool) – whether the output tensor has dim retained or not.

Returns the sum of each row of the input tensor in the given dimension dim.

		For a 2*3 matrix we can think like the first dimension (dim=0) stays for rows and the second one 
(dim=1) for columns. Following the reasoning we can assume that the dimension dim=0 means row-wise, the
reality was the opposite of what we can expect. dim=0 means column wise, dim=1 means rowwise. Similar to
NumPy matrices where we pass a second parameter called axis. NumPy sum is almost identical to what
we have in PyTorch except that dim in PyTorch is called axis in NumPy.

		it becomes trickier when we introduce a third dimension. When we look at the shape of a 3D tensor 
we’ll notice that the new dimension gets prepended and takes the first position (in bold below) i.e.
the third dimension becomes dim=0.

	For dim=0 in 3-D matrix, The first dimension (dim=0) of this 3D tensor is the highest one and 
contains 3 two-dimensional tensors. we have to collapse the 2-D tensors.
	For dim=1 in 3-D matrix, we have to collapse the rows.
	For dim=2 in 3-D matrix, we have to collapse the columns.
The minus essentially means you go backwards through the dimensions. Let A be a n-dimensional matrix.
Then dim=n-1=-1, dim=n-2=-2, ..., dim=1=-(n-1), dim=0=-n. 

Example for torch.sum :
	In [210]: X
	Out[210]: 
	tensor([[  1,  -3,   0,  10],
			[  9,   3,   2,  10],
			[  0,   3, -12,  32]])

	In [211]: X.sum(1)
	Out[211]: tensor([ 8, 24, 23])

	In [212]: X.sum(0)
	Out[212]: tensor([ 10,   3, -10,  52])
	
	As, we can see from the above outputs, in both cases, the output is a 1D tensor. If you, on the other hand, wish to retain
the dimension of the original tensor in the output as well, then you've set the boolean kwarg keepdim to True as in:

	In [217]: X.sum(0, keepdim=True)
	Out[217]: tensor([[ 10,   3, -10,  52]])

	In [218]: X.sum(1, keepdim=True)
	Out[218]: 
	tensor([[ 8],
			[24],
			[23]])
			
------------------------------------------------------------------
difference between the torch.stack and torch.cat ?

So, what is the basic difference between these two methods? torch.cat() concatenates a sequence of tensors along an existing dimension, 
hence not changing the dimension of the tensors. torch. stack() stacks the tensors along a new dimension, as a result, 
it increases the dimension.

stack

	Concatenates sequence of tensors along a new dimension.

cat

	Concatenates the given sequence of seq tensors in the given dimension.

So if A and B are of shape (3, 4):

torch.cat([A, B], dim=0) will be of shape (6, 4)
torch.stack([A, B], dim=0) will be of shape (2, 3, 4)



------------------------------------------------------------------			
			
	torch.argmin(input, dim=None, keepdim=False) → LongTensor
	
	Returns the indices of the minimum value(s) of the flattened tensor or along a dimension

	NOTE: If there are multiple minimal values then the indices of the first minimal value are returned.

	Parameters:
		input (Tensor) – the input tensor.
		dim (int) – the dimension to reduce. If None, the argmin of the flattened input is returned.
		keepdim (bool) – whether the output tensor has dim retained or not. Ignored if dim=None.

Example:

	>>> a = torch.randn(4, 4)
	>>> a
	tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],
			[ 1.0100, -1.1975, -0.0102, -0.4732],
			[-0.9240,  0.1207, -0.7506, -1.0213],
			[ 1.7809, -1.2960,  0.9384,  0.1438]])
	>>> torch.argmin(a)
	tensor(13)
	>>> torch.argmin(a, dim=1)
	tensor([ 2,  1,  3,  1])
	>>> torch.argmin(a, dim=1, keepdim=True)
	tensor([[2],
			[1],
			[3],
			[1]])

-----------------------------------------------------------------------------			
torch.argmax(input) → LongTensor
	Returns the indices of the maximum value of all elements in the input tensor.

	NOTE:If there are multiple maximal values then the indices of the first maximal value are returned.

	Parameters
	input (Tensor) – the input tensor.

	Example:

	>>> a = torch.randn(4, 4)
	>>> a
	tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
			[-0.7401, -0.8805, -0.3402, -1.1936],
			[ 0.4907, -1.3948, -1.0691, -0.3132],
			[-1.6092,  0.5419, -0.2993,  0.3195]])
	>>> torch.argmax(a)
	tensor(0)
	torch.argmax(input, dim, keepdim=False) → LongTensor
	Returns the indices of the maximum values of a tensor across a dimension.

	Parameters
	input (Tensor) – the input tensor.

	dim (int) – the dimension to reduce. If None, the argmax of the flattened input is returned.

	keepdim (bool) – whether the output tensor has dim retained or not. Ignored if dim=None.

	Example:

	>>> a = torch.randn(4, 4)
	>>> a
	tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
			[-0.7401, -0.8805, -0.3402, -1.1936],
			[ 0.4907, -1.3948, -1.0691, -0.3132],
			[-1.6092,  0.5419, -0.2993,  0.3195]])
	>>> torch.argmax(a, dim=1)
	tensor([ 0,  2,  0,  1])

---------------------------------------------------------------------------

	torch.mean(input, dim, keepdim=False, *, dtype=None, out=None)

Returns the mean value of each row of the input tensor in the given dimension dim. If dim is a list
of dimensions, reduce over all of them.

If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim
where it is of size 1.Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor
having 1

----------------------------------------------------------------------------
torch.randn(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
	
	Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 
(also called the standard normal distribution).

	out ~ N(0,1)
	
	The shape of the tensor is defined by the variable argument size.
	
	Parameters:
	
		size (int...) – a sequence of integers defining the shape of the output tensor.
Can be a variable number of arguments or a collection like a list or tuple.

	Keyword Arguments:
	
		generator (torch.Generator, optional) – a pseudorandom number generator for sampling

		out (Tensor, optional) – the output tensor.

		dtype (torch.dtype, optional) – the desired data type of returned tensor. Default: if None, uses a
global default (see torch.set_default_tensor_type()).

		layout (torch.layout, optional) – the desired layout of returned Tensor. Default: torch.strided.

		device (torch.device, optional) – the desired device of returned tensor. Default: if None, uses the current device
for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current
CUDA device for CUDA tensor types.

		requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.
----------------------------------------------------------------------------

nn.Embedding(num_embedding,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,_weight=None,device=None,dtype=None)

A simple lookup table that stores the embeddings of a fixed dictionary and size.

	This module is often used to store word embeddings and retrive them using indices. The input to the module is a list of 
indices and output is the corresponding word embeddings.

num_embeddings (int) – size of the dictionary of embeddings

embedding_dim (int) – the size of each embedding vector

padding_idx (int, optional) – If specified, the entries at padding_idx do not contribute to the gradient; therefore, the embedding vector at padding_idx is not updated during training, i.e. it remains as a fixed “pad”. For a newly constructed Embedding, the embedding vector at padding_idx will default to all zeros, but can be updated to another value to be used as the padding vector.

max_norm (float, optional) – If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm.

norm_type (float, optional) – The p of the p-norm to compute for the max_norm option. Default 2.

scale_grad_by_freq (boolean, optional) – If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False.

sparse (bool, optional) – If True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.
-----------------------------------------------------------------------------
Pytorch:-
		1. squeeze - removes dimensions whichever are size 1 in the shape of the torch tensor and 
compresses it into compact tensor.It will just eliminate the dimension at the specific index if dim=1.
If not, it won’t change anything.

		2. unsqueeze - it inserts a new dimension of size 1 at the specified position of dim and 
returns the tensor.
		3. view - The view function is meant to reshape the tensor.Drawing a similarity between numpy and
pytorch, view is similar to numpy's reshape function
eg: Suppose we have a tensor a with 16 elements and we want to reshape it to 4*4 tensor we use view(4,4).
	import torch
	a = torch.range(1, 16)
	a = a.view(4,4)
Note:- after the reshape the total number of elements need to remain the same. Reshaping the tensor a 
to a 3 x 5 tensor would not be appropriate.

Meaning of parameter -1 in view:
	If there is any situation that you don't know how many rows you want but are sure of the number of
columns, then you can specify this with a -1. (Note that you can extend this to tensors with more
dimensions. Only one of the axis value can be -1). This is a way of telling the library: "give me a 
tensor that has these many columns and you compute the appropriate number of rows that is necessary to
make this happen".

The view(-1) operation flattens the tensor, if it wasn’t already flattened as seen here:

	x = torch.randn(2, 3, 4)
	print(x.shape)
	O/P: torch.Size([2, 3, 4])
	x = x.view(-1)
	print(x.shape)
	O/P: torch.Size([24])

It’ll modify the tensor metadata and will not create a copy of it.

		4. all - checks if all the boolean condition in the specified axis is True or Not. returns: Boolean array along the specified 
axis if keepdim is True otherwise returns array along the other dimension. 
		
		5. permute - pemutes the order of tensor shape given in the order accordingly
		
		a = torch.randn(4,3,5)
		a.permute(2,1,0)
		a.shape #(5,3,4)
		
-----------------------------------------------------------------------------

Tensor.detach():
	
	This method is used to detach a tensor from the current computational graph. It returns a new tensor that doesn't require a 
gradient.

	When we don't need a tensor to be traced for the gradient computation, we detach the tensor from the current computational 
graph.
	We also need to detach a tensor when we need to move the tensor from GPU to CPU.

It returns a new tensor without requires_grad = True. The gradient with respect to this tensor will no longer be computed.
-----------------------------------------------------------------------------

Yeild Function and Generators in Python:
		
		Yeild will suspend the function execution and stores the local variables in memory and returns the
last executed value.
Benefits of yield: 
1. Using a yield statement in a function definition is sufficient to cause that definition to create a 
generator function instead of a normal function. 
2. We should use yield when we want to iterate over a sequence but don't want to store the entire 
sequence in memory 
3. yield is used in Python generators. A generator function or expression will defined like a 
normal function, but whenever it needs to generate a value, it does so with the yield keyword rather than
return.


https://www.knowledgehut.com/blog/programming/yield-in-python

math.floor(x) - this function returns the floor value of x.
|_x_| is represented as floor of x in mathematics.
|_x_| <= x(gives value lesser than or equal to x). 
eg:- 
floor(2) = floor(2.00001) = floor(2.9999) = 2, 
floor(3) = floor(3.00001) = floor(3.9999) = 3,
floor(-2) = floor(-1.001) = floor(-1.999) = -2,
floor(-3) = floor(-2.001) = floor(-2.999) = -3 
floor of any number between 2 to 3 is 2 only.

math.ceil(x) - this function returns the cieling value of x.
eg:
ceil(2) = ceil(2.4) = ceil(2.999) = 3. 


Modulo − represents as % operator. 
Modulo gives the value of the remainder of an integer division.

Floor Division  - represents as // operator.
Floor Division gives the integer value of the quotient of a division.
Floor Division always gives the floor of the division.
 
Division − represents as / operator. 
Division gives the floor value of the quotient of a division. 
Division always gives the floating point value of the division.

Collections:-
1. Counters - 
2. OrderedDict -
3. DefaultDict -
4. ChainMap -
5. NamedTuple -
6. Deque -
7. UserDict -
8. UserList - 
9. UserString -

Copy Function in Python:

copy.copy(x):
Return a shallow copy of x.

copy.deepcopy(x[, memo]):
Return a deep copy of x.

Minimum window Substring:
1. Given: two strings s,t of length m and n. 
2. return minimum window substring of s such that every character of t is included (duplicates allowed) in the window
3. for no such substring return "" empty string)
4. s and t contains both lower case and upper case letters.

Breadth First Traversal (Queue)

Diff Types of Depth First Traversal in Trees(Stack): 123 -> 213 -> 231.

PreOrder Traversal - Root Left Right (RoLeRi) 123
PostOrder Traversal - Left Right Root (LeRiRo) 231
InOrder Traversal - Left Root Right (LeRoRi) 213


In a binary tree, a node can have maximum two children. (All the rules in BST are same as in binary tree 
and can be visualized in the same way.)

Calculating minimum and maximum height from number of nodes – 
If there are n nodes in binary tree, maximum height of the binary tree is n-1 and minimum height 
is floor(log2n). 

			log2n = log10(n)/log10(2) = 3.322*log10(n).

Calculating minimum and maximum number of nodes from height – 
If binary tree has height h, minimum number of nodes is h+1 (in case of left skewed and right skewed 
binary tree). 

For example, for the binary tree with height 2 has 3 nodes.If binary tree has height h, maximum number 
of nodes will be when all levels are completely full.Total number of nodes will be 
2^0 + 2^1 + …. 2^h = 2^(h+1)-1. 

Ascii Values:
a - 97  A - 65 0 - 48
z - 122 Z - 90 9 - 57
ord() - character to byte encoding.
chr() - byte encoding to character conversion.

When you compare chars, their ordinal values are compared

So 'a' < 'b' just means ord('a') < ord('b')

Deque Methods:
1. append -  add value to right side of deque.
2. appendleft - add value to left side of deque.
3. pop - remove value from right side of deque.
4. popleft - remove value from left side of deque.
5. index(ele, beg, end) - This function returns the first index of the value mentioned in arguments, 
starting from searching start index till end index.
6. insert(i, a) - This function inserts the value mentioned in arguments(a)
at index(i) specified in arguments.
7. remove - This function removes the first occurrence of value mentioned in arguments.
8. count - This function counts the number of occurrences of value mentioned in arguments.
9. extend(iterable) - This function is used to add multiple values at the right end of the deque. The argument passed is iterable.
10. extendleft(iterable) - This function is used to add multiple values at the left end of the deque. The argument passed is iterable. Order is reversed as a result of left appends.
11. reverse - This function is used to reverse the order of deque elements.
12. rotate(i) - This function rotates the deque by the number i specified in arguments. 
If i is negative, rotation occurs to the left. Else rotation is to right.


Set Methods:
1. add -  adds element to a set
2. copy - if you need the original set to be unchanged when the new set is modified,
you can use the copy() method. if =(equals) is used then whatever changes in copied set done
is reflected in original set.
3. clear - The clear method removes all the elements from the set.
4. difference - The difference() method returns the set difference of two sets.
	If A = {1, 2, 3, 4}
	   B = {2, 3, 9}
	Then,
	   A - B = {1, 4}
       B - A = {9}
5. intersection - The intersection() method returns a new set with elements 
that are common to all sets. (& operator - shortcut).
A.intersection(*other_sets)
	eg:
		A = {2, 3, 5, 4}
		B = {2, 5, 100}
		C = {2, 3, 8, 9, 10}

		print(B.intersection(A)) - {2, 5}
		print(B.intersection(C)) - {2}
		print(A.intersection(C)) - {2, 3}
		print(C.intersection(A, B)) - {2}
6. intersection-update - The intersection_update() updates the set calling 
intersection_update() method with the intersection of sets.
7. isdisjoint - The isdisjoint() method returns True if two sets are disjoint sets.
If not, it returns False.
8. issubset - returns true if one set is subset of another set.
9. pop - The pop() method removes an arbitrary element from the set 
and returns the element removed.
10. union - This method returns a new set with distinct elements from all the sets.
The union() method returns a new set with elements from the set
and all other sets (passed as an argument). (| operator - shortcut).
If the argument is not passed to union(), it returns a shallow copy of the set.
11. symmetric_difference() - Symmetric Difference of A and B is a set of elements in A and B but not in
 both (excluding the intersection). (^ operator - shortcut).
12. all()	Returns True if all elements of the set are true (or if the set is empty).
13. any()	Returns True if any element of the set is true. If the set is empty, returns False.
14. remove()	Removes an element from the set. If the element is not a member, raises a KeyError.

FrozenSet:

Frozenset is a new class that has the characteristics of a set, but its elements cannot be changed once 
assigned. While tuples are immutable lists, frozensets are immutable sets.

Sets being mutable are unhashable, so they can't be used as dictionary keys. On the other hand,
frozensets are hashable and can be used as keys to a dictionary.Frozensets can be created using the 
frozenset() function.

This data type supports methods like copy(), difference(), intersection(), isdisjoint(), issubset(),
issuperset(), symmetric_difference() and union(). Being immutable, it does not have methods that add
or remove elements.

-------------------------------------------------------------------------------------------------
Map Methods:

map.get(key,default_value):

Example:

	It allows you to provide a default value if the key is missing:

	dictionary.get("bogus", default_value)
	returns default_value (whatever you choose it to be), whereas

	dictionary["bogus"]
	would raise a KeyError.

	If omitted, default_value is None, such that

	dictionary.get("bogus")  # <-- No default specified -- defaults to None
	returns None just like dictionary.get("bogus", None) would.
-------------------------------------------------------------------------------------------------
Numpy Functions:

Numpy quantile Func:
			numpy.quantile(arr, q, axis = None)
Parameters :
	arr : [array_like]input array.
	q : quantile value.	
	axis : [int or tuples of int] (axis along which we want to calculate the quantile value. 
		    Otherwise, it will consider arr to be flattened(works on all the axis). 
			axis = 0 means the column and axis = 1 means along the row.)
	
Output:
	out : [ndarray, optional] (Different array in which we want to place the result.
		   The array must have same dimensions as expected output.)
		   
np.argsort() - Return the indices of the sorted array to sort the array.

Numpy matrix Reshape:

z = np.array([[1, 2, 3, 4],
			  [5, 6, 7, 8],
			  [9, 10, 11, 12]])

z.shape
(3, 4)

z.reshape(-1)
array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])

z.reshape(-1,1)
Output:   array([[ 1],
				 [ 2],
				 [ 3],
				 [ 4],
				 [ 5],
				 [ 6],
				 [ 7],
				 [ 8],
				 [ 9],
				 [10],
				 [11],
				 [12]])
   
z.reshape(-1, 2)
Output:  array([[ 1,  2],
			   [ 3,  4],
               [ 5,  6],
			   [ 7,  8],
			   [ 9, 10],
			   [11, 12]])  

z.reshape(1,-1)
array([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]])

z.reshape(2, -1)
array([[ 1,  2,  3,  4,  5,  6],
   [ 7,  8,  9, 10, 11, 12]])

z.reshape(3, -1)
array([[ 1,  2,  3,  4],
   [ 5,  6,  7,  8],
   [ 9, 10, 11, 12]])

And finally, if we try to provide both dimension as unknown i.e new shape as (-1,-1). It will throw an error

z.reshape(-1, -1)
ValueError: can only specify one unknown dimension.

np.size - count no of elements in numpy array.

Eye Function:

	numpy.eye(N, M=None, k=0, dtype=<class 'float'>, order='C', *, like=None)[source]
	
	Return a 2-D array with ones on the diagonal and zeros elsewhere.

	Parameters
	N - Number of rows in the output.

	M - Number of columns in the output. If None, defaults to N.

	k - Index of the diagonal: 0 (the default) refers to the main diagonal, a positive value refers to
an upper diagonal, and a negative value to a lower diagonal.

example:

np.eye(3, k=1)

O/P:	array([[0.,  1.,  0.],
			   [0.,  0.,  1.],
			   [0.,  0.,  0.]])

What Is a Joint Probability?

Joint probability is a statistical measure that calculates the likelihood of 
two events occurring together and at the same point in time.
Joint probability is the probability of event Y occurring at the same time 
that event X occurs.

Joint probability only factors the likelihood of both events occurring.
Conditional probability can be used to calculate joint probability, 
as seen in this formula:

P(X and Y) = P(X|Y)*P(Y)P(X∩Y) = P(X∣Y)×P(Y)


GridSearchCV:

GridSearchCV is the process of performing hyperparameter tuning in order to determine the optimal values for a given model. As mentioned above, 
the performance of a model significantly depends on the value of hyperparameters.

GridSearchCV tries all the combinations of the values passed in the dictionary and evaluates the model for each combination 
using the Cross-Validation method. Hence after using this function we get accuracy/loss for every combination of hyperparameters and we can
choose the one with the best performance.

sklearn.model_selection.GridSearchCV(estimator, param_grid,scoring=None,
          n_jobs=None, iid='deprecated', refit=True, cv=None, verbose=0, 
          pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)
		  
1.estimator: Pass the model instance for which you want to check the hyperparameters.
2.params_grid: the dictionary object that holds the hyperparameters you want to try
3.scoring: evaluation metric that you want to use, you can simply pass a valid string/ object of evaluation metric
4.cv: number of cross-validation you have to try for each selected set of hyperparameters
5.verbose: you can set it to 1 to get the detailed print out while you fit the data to GridSearchCV
6.n_jobs: number of processes you wish to run in parallel for this task if it -1 it will use all available processors.		  


Bayes Theoram:

		Bayes theoram is a principled way of calculating conditional probability 
without joint probability. Bayesian theoram accounts for the alternate conditional probability
calculation without joint probability(which is challenging to calculate) or when 
reverse conditional probability is available or easy to calculate.

					P(A|B) = P(A,B)/P(B) = (P(B|A)*P(A))/P(B)
									(or)
					P(B|A) = P(B,A)/P(A) = (P(A|B)*P(B))/P(A)
	 
	 P(A) -  The Probability of A occuring.
	 P(B) -  The Probability of B occuring.
	 P(A|B) - The probability of A occured given B.
	 P(B|A) -  The probability of B occured given A.
	 P(A intersect B) -  The probability of both A and B occuring.
P(A intersect B)= P(A)*P(B|A).

Marginal Probability - The probability of an event irrespective of the outcome of 
other random variables P(A),P(B).

	P(A) - P(A|B)*P(B) + P(A|not B)*P(not B),  P(not B) = 1 - P(B), P(A|not B) = 1 - P(not A|not B).
	P(B) - P(B|A)*P(A) + P(B|not A)*P(not A),  P(not A) = 1 - P(A), P(B|not A) = 1 - P(not B|not A).

Joint Probability -  The probability of two or more simultaneous events 
eg. P(A and B) = P(A,B), P(B and A) = P(B,A). P(A,B)=P(A|B)*P(B).

Conditional Probability -  The probability of one event given the occurence of another event.
eg. P(A given B) or P(A|B).


Expected Value in Probability & Statistics:

	In probability theory the expected value (also called expectation) is a generalization of the weighted average. Informally,
the expected value is the arithmetic mean of a large number of independently selected outcomes of a random variable.

Expectation of continuous random variable:
		E(x) = Integral{-Inf to Inf} [x*P(x)]dx
		
		E(x) - expectation value of continuous random Variable X
		x - value of continuous random Variable X.
		P(x) - probability density function.
		

Expectation of discrete random variable:

		E(x) = Summ{i}(xi*P(x))
		
		E(x) - Expectation value of discrete random variable X.
		x - value of discrete random variable.
		P(x) - probability mass function of x.
		
Properties of Expectation:
	Linearity:
		E(aX) = aE(x)
		E(X+Y) = E(x) + E(Y)
		
	Product:
		E(X.Y) = E(X).E(Y)
	conditional expectation.
	
Example: purchase 10$ ticket, 200 tickets sold, and 1st prize car, 2nd price CD player, 3rd prize - Luggage Set.

						Win			Win			Win		  Lose
						Car		   CD Player   Luggage	  Ticket Cost
	Gain(x)			  14900$		100$        200$		-10$
	Probability P(x)    1/200		1/200		1/200	  197/200
	
Then multiply and add probabilities = 14900*(1/200)+100*(1/200)+200*(1/200)+-10*(197/200) = 66.15 (Expected Value).

Prior,Posterior Probability and Likelihood:

			The terms in the Bayes Theoram equation can also be alternatively described as:

		P(A|B) or P(B|A) = Posterior Probability(the left side term of the bayes theoram).
		P(B|A) or P(A|B) = Likelihood(The first term on right side or reverse 
		conditional probability).
		P(A) or P(B) = Prior Probability(The second term on right side multiplied 
		with likelihood).
		P(B) or P(A) = Normalizing Factor or Evidence(The denominator part).

Prior refers to whatever preconcieved notions or beliefs that we hold.

Likelihood refers to the probability of observing what we did given that our priors are true.

Posterior refers to updated prior conditioned on what we have observed.

Normalizing factor refers to the evidence and is used to ensure that posterior is a 
probability and is not above 1.



Lagrange Multipliers:

the method of Lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to equality constraints (i.e., subject to the condition that one or more equations have to be satisfied exactly by the chosen values of the variables).[1] It is named after the mathematician Joseph-Louis Lagrange. The basic idea is to convert a constrained problem into a form such that the derivative test of an unconstrained problem can still be applied. The relationship between the gradient of the function and gradients of the constraints rather naturally leads to a reformulation of the original problem, known as the Lagrangian function.[2]

The method can be summarized as follows: in order to find the maximum or minimum of a function {\displaystyle f(x)}f(x) subjected to the equality constraint {\displaystyle g(x)=0}g(x)=0, form the Lagrangian function

{\displaystyle {\mathcal {L}}(x,\lambda )=f(x)+\lambda g(x)}{\displaystyle {\mathcal {L}}(x,\lambda )=f(x)+\lambda g(x)}
and find the stationary points of {\displaystyle {\mathcal {L}}}{\mathcal {L}} considered as a function of {\displaystyle x}x and the Lagrange multiplier {\d


				Lagrange multipliers is a simple and elegant method for calculating the local
minima or local maxima of a function subject to equality and inequality constraints. 

		Suppose we have the following optimiztion problem:
							Minimize F(x)
						subject to w.r.t. condition, 
						g_1(x)=0,g_2(x)=0,g_3(x)=0....g_n(x)=0
						
						L(x,y) = F(x) + L_1*g_1(x) + L_2*g_2(x) + L_3*g_3(x)+....+L_n*g_n(x)
						
						L_v = [L_1,L_2,L_3,....,L_n] where L_v represents vector of Lagrange 
						multipliers.



Variance, Covariance and Correlation Coefficient Definition and examples:

Before you start the machine learning process, it is critical to prepare your data so that only the relevant parts of your
dataset is used for training. To understand the relationships in your dataset, you need to understand the 
following concepts:

	1.Variance
	2.Covariance
	3.Correlation
	


Variance:
	Variance is the spread of values in a dataset around its mean value. It tells you how far each number in the dataset 
is from its mean.
	
		s = summation(xi - x(mean))**2//(n-1)  -> correlation.
		
	For sample variance, the denominator is n-1. For population variance, the denominator is n.

For sample variance, the denominator is n-1. For population variance, the denominator is n.


The square root of variance (s²) is the standard deviation (s). Variance is calculated by taking the difference of each number
in the dataset from the mean, summing all the differences, and finally dividing it by the number of values in the dataset.

A large variance indicates that the numbers in the dataset are far from the mean and far from each other. A small variance, 
on the other hand, indicates that the numbers are close to the mean and to each other. A variance of 0 indicates that all 
the numbers in the dataset are the identical. Finally, the valid value of variance is always a positive number (0 or more).


Covariance:

Covariance is a measure of how much two random variables vary together. It’s similar to variance, 
but where variance tells you how a single variable varies, co variance tells you how two variables vary together.

While variance measures the spread of data within its mean value, 
covariance measures the relationalship between two random variables.In statistics, covariance is the measure of the directional
relationship between two random variables.

In statistics, a and b are known to have a positive covariance. A positive covariance indicates that both random variables
tend to move upward or downward at the same time.

In statistics, b and c are known to have a negative covariance. A negative covariance indicates that both variables tend 
to move away from each other — when one moves upward the other moves downward, and vice versa.

In statistics, c and d are known to have zero covariance (or close to zero). When two random variables are independent,
the covariance will be zero. However, the reverse is not necessarily true — a covariance of zero does not mean that 2 random 
variables are independent (a non-linear relationship can still exist between 2 random variables that has zero covariance). 
In the above example, you can see that there exists some sort of non-linear v-shape relationship.


Note on dividing by n or n-1:

When dealing with samples, there are n-1 terms that have the freedom to vary (see: Degrees of Freedom). 
If you are finding the covariance of just two random variables, just divide by n.

var(x) = ((xi-x(mean))*(xi-x(mean)))/(n-1) -> variance

cov(x,y) = (xi - x(mean))*(yi - y(mean)) // (n-1) -> covariance.

Covariance between 2 random variables is calculated by taking the product of the difference between the value of each 
random variable and its mean,summing all the products, and finally dividing it by the number of values in the dataset.

r(xy) = cov(x,y)/s(x)*s(y) =
 (summation(xi-x(mean))*summation(yi-y(mean)))/(sqrt(summation(xi-x(mean)))*sqrt(summation(yi-y(mean))))
 
 
Correlation:

	Correlation is a statistical technique that is used to measure and describe a relationship between the two variables. Usually the two variables are simply observed, not manipulated.
	
Correlations have three important characterstics. they can tell us about the direction of the relationship, the form (shape) of the relationship and the degree (strength) of the 
relationship between two variables.

1. The Direction of a Relationship: The correlation measure tells us about the direction of the relationship between the two variables. The direction can be positive or negative.

Positive In a positive relationship, both variables tend to move in the same direction. if one variable increases, the other also tends to increases.If one decreases, the other tends to decrease.

Negative In a negative relationship, the variables tend to move in the opposite direction. If one variable tends to increase then the other tends to decrease.

The direction of the relationship between two variables is identified by the sign of the correlation coefficient for the variables. Postive relationships have a "plus" sign,
whereas negative relationships have a "minus" sign.

2. The Form (Shape) of a Relationship: The form or shape of a relationship refers to whether the relationship is straight or curved.

Linear: A straight relationship is called linear, because it approximates a straight line.

Curvilinear: A curved relationship is called curvilinear, because it approximates a curved line.

3. The Degree (Strength) of a Relationship: Finally, a correlation coefficient measures the degree(strength) of the relationship between the two variables. the measures we discuss only measure the strength of the 
linear relationship between the two variables. Two specific strengths are:
	
Perfect Relationship:	when two variables are exactly(linear) related the correaltion coefficient is either +1 or -1. They are said to be perfectly linearly related either positively or negatively.

No Relationship: when two variables are having no relationship at all, their correlation is 0.0. 

Where & Why we use Correlation

Prediction: Correlations can be used to help make predictions. If two variables have been known in the past to correlate, then we can assume they will continue to correlate in the future. 
We can use the value of one variable that is known now to predict the value that the other variable will take on in the future.

Relaibility: Correlations can be used to determine the reliability of some measurement process. For example, we could administer our new IQ test on two different occasions to the same group 
of people and see what the correlation is. If the correlation is high, the test is reliable. If it is low, it is not.
 
Pearson’s Correlation Coefficient(Linear Association)– 
		
		Covariance standardized for variance (covariance divided by product of standard deviations)

r(xy) = cov(x,y)/s(x)*s(y) = (xi - x(mean)) * (yi - y(mean)) / sqrt(summation(x(i)-x(mean))) * sqrt(summation(y(i)-y(mean)))

The Pearson correlation coefficient is just a normalized covariance between the two variables to give an interpretable score such that  Corrp(x,y)∈[−1,1]. It can be used to summarize the strength of the linear
relationship between two data samples. The use of mean and standard deviation in the calculation suggests the need for the two data samples to have a Gaussian or Gaussian-like distribution 
hence it's a parametric statistic.

As a statistical hypothesis test, the method assumes that the samples are uncorrelated (fail to reject H0).

Assumptions of pearson correlation:
	1. Both variables should have a Gaussian or Gaussian-like distribution.
	2. Relationship between the variables should be linear.
	3. Homoscedasticity i.e., a sequence of random variables where all its random variables have the same finite variance.
	
Also Pearson is quite sensitive to outliers.

Removing features with zero or very low correlation: Features that show little or no correlation with the target variable may not contribute much to the
predictive power of a model. Removing such features can help reduce noise and simplify the model. 
The specific threshold for "low correlation" depends on the context and problem at hand, but a common threshold is a correlation 
coefficient below 0.3 or 0.4.

Exploring highly correlated features: When two features have a high positive or negative correlation (close to +1 or -1) with each other, it indicates that they
are strongly related and may provide redundant information. In such cases, it can be beneficial to explore and potentially remove one of the highly correlated 
features to avoid multicollinearity, which can cause instability or overfitting in certain models. However, the decision to remove a highly correlated 
feature should be based on the context and domain knowledge. Sometimes, both features may still be relevant and capture different aspects of the target variable.

Ordinal Association (Rank correlation):

	Two variables may be related by a nonlinear relationship, such that the relationship is stronger or weaker across the distribution of the variables. In these cases, even when variables have a strong association, 
Pearson’s correlation would be low. Further, the two variables being considered may have a non-Gaussian distribution. To properly identify association between variables with non-linear relationships,
we can use rank-based correlation approaches.

	Rank correlation refers to the methods that quantify the association between the variables using the ordinal relationship between the values rather than the specific values like in Pearson's coefficient.
(ie). rank correlation focuses on the ordinal relationship or the order of the values.

	When variables are measured on an ordinal scale, which means the values have a natural ordering or ranking (e.g., small, medium, large), rank correlation becomes useful. Instead of considering the specific values, 
these methods examine the relative positions or ranks of the values. The underlying assumption is that even though the actual values might differ, the ordinal relationship or the order of the values remains meaningful.

	By comparing the rankings of the variables, rank correlation methods provide a measure of how closely the variables are related.these methods are particularly suited for situations where the data is not normally
distributed.These methods are particularly suitable for situations where the data are not normally distributed or when the variables are measured on an ordinal scale rather than a continuous scale.

Overall, rank correlation allows us to assess the association between variables based on their ordinal relationship.Four types of rank correlation methods are as follows:

1. Spearman's Rank Correlation:
	
		Corr(x,y) = Cov(rank(x),rank(y))/sigma_rank(x)*sigma_rank(y).
	
	Spearman's Correlation is a non-parametric rank correlation and is also interpretable because  Corrs(x,y)∈[−1,1] . In this instead of calculating the coefficient using covariance and 
standard deviations on the samples themselves, these statistics are calculated by converting the raw data into rank data hence non-parametric. This is a common approach used in non-parametric statistics.

	As a statistical hypothesis test, the method assumes that the samples are uncorrelated (fail to reject H0).

2. Kendall's Rank Correlation:	
	
	The intuition for the test is that it calculates a normalized score for the number of matching or concordant rankings between the two samples. As a statistical hypothesis test, 
the method assumes that the samples are uncorrelated (fail to reject H0).

3. Goodman and Kruskal’s Rank Correlation:

4. Somers’ Rank Correlation

	Types of Correlation:
		Positive: both variables change in the same direction.
		Neutral: no relationship in the change of the variables.
		Negative: variables change in opposite directions.
	
--------------------------------------------------------------------------------------------------------------------------------
Parametric models are those that require the specification of some parameters before they can be used to make predictions,
while non-parametric models do not rely on any specific parameter settings and therefore often produce more accurate results.

What’s the difference between parametric and non-parametric models?

The following is the list of differences between parametric and non-parametric machine learning models.

In case of parametric models, the assumption related to the functional form is made and linear model is considered.
In case of non-parametric models, the assumption about the functional form is not made.

Parametric models are much easier to fit than non-parametric models because parametric machine learning models only require
the estimation of a set of parameters as the model is identified prior as linear model. In case of non-parametric model, one
needs to estimate some arbitrary function which is a much difficult task.

Parametric models often do not match the unknown function we are trying to estimate. The model performance is comparatively 
lower than the non-parametric models. The estimates done by the parametric models will be farther from being true.

Parametric models are interpretable unlike the non-parametric models. This essentially means that one can go for parametric
models when the goal is to find inference. Instead, one can choose to go for non-parametric models when the goal is to make
prediction with higher accuracy and interpretability or inference is not the key task.

Such models are called as parametric machine learning models. The parametric models are linear models which includes 
determining the parameters such as that shown above. The most common approach to fitting the above model is referred to as
ordinary least squares (OLS) method. However, least squares is one of many possible ways to fit the linear model. Example of
parametric models include linear algorithms such as Lasso regression, linear regression and to an extent, generalized additive
models (GAMs).

Building non-parametric models do not make explicit assumptions about the functional form such as linear model in case of 
parametric models. Instead non-parametric models can be seen as the function approximation that gets as close to the data 
points as possible. The advantage over parametric approaches is that by avoiding the assumption of a particular functional 
form such as linear model, non-parametric models have the potential to accurately fit a wider range of possible shapes for 
the actual or true function. Any parametric approach brings with it the possibility that the functional form (linear model) 
which is very different from the true function, in which case the resulting model will not fit the data well. Example of 
non-parametric models include fully non-linear algorithms such as bagging, boosting, support vector machines bagging boosting
with non-linear kernels, and neural networks (deep learning).

When the goal is to achieve models with high performance prediction accuracy, one can go for non-linear methods such as bagging,
boosting, support vector machines bagging boosting with non-linear kernels, and neural networks (deep learning). When the goal
is to achieve modeling for making inferences, one can go for parametric methods such as lasso regression, linear regression etc
which have high interpretability.


--------------------------------------------------------------------------------------------------------------------------------
Clustering:

Introduction

In supervised learning, K-Means Clustering is a simple yet powerful algorithm in data science
There are a plethora of real-world applications of K-Means Clustering (a few of which 
we will cover here) This comprehensive guide will introduce you to the world of clustering 
and K-Means Clustering along with an implementation in Python on a real-world dataset

I love working on recommendation engines.Whenever I come across any recommendation engine
on a website, I can’t wait to break it down and understand how it works underneath.
It’s one of the many great things about being a data scientist!

What truly fascinates me about these systems is how we can group similar items, products, 
and users together. This grouping, or segmenting, works across industries. 
And that’s what makes the concept of clustering such an important one in data science.

Clustering helps us understand our data in a unique way – by grouping things together into 
– you guessed it – clusters. k_means_clustering:

In this article, we will cover k-means clustering and it’s components comprehensively. 
We’ll look at clustering, why it matters, its applications and then deep dive into 
k-means clustering (including how to perform it in Python on a real-world dataset).

What is Clustering?
Let’s kick things off with a simple example. 
A bank wants to give credit card offers to its customers. 
Currently, they look at the details of each customer and based on this information, 
decide which offer should be given to which customer.

Now, the bank can potentially have millions of customers.
Does it make sense to look at the details of each customer separately and then make a decision?
Certainly not! It is a manual process and will take a huge amount of time.

So what can the bank do? One option is to segment its customers into different groups. 
For instance, the bank can group the customers based on their income:

customer segmentation
Can you see where I’m going with this? The bank can now make three different strategies or offers, 
one for each group. Here, instead of creating different strategies for individual customers, 
they only have to make 3 strategies. This will reduce the effort as well as the time.

The groups I have shown above are known as clusters and the process of
 creating these groups is known as clustering. Formally, we can say that:

Clustering is the process of dividing the entire data into groups 
(also known as clusters) based on the patterns in the data.

Can you guess which type of learning problem clustering is? 
Is it a supervised or unsupervised learning problem?

Think about it for a moment and make use of the example we just saw. 
Got it? Clustering is an unsupervised learning problem!

 

How is Clustering an Unsupervised Learning Problem?
Let’s say you are working on a project where you need to predict the sales of a big mart:

In supervised learning, We have a fixed target to predict.So, we have a target variable to predict 
based on a given set of predictors or independent variables.
 
In unsupervised learning, we do not have a target to predict. 
We look at the data and then try to club similar observations and form different groups. 

Properties of Clustering: 
1. All the data points in a cluster should be similar to each other.
2. The data points from different clusters should be as different as possible.		



K-means++ Algorithm:
https://www.geeksforgeeks.org/ml-k-means-algorithm/

Eigen Values and Eigen Vectors:

Matrices acts on vectors and multiplies vector X. 
The way matrix acts is (in goes a vector X and out comes a vector
AX) like a function with multiple dimensions.

The vectors we are specially interested in are the vectors 
that come out in the same direction parallely. These vectors are called EigenVectors.

EigenVectors are AX vectors parallel to X. ie. AX = (Lamda)X -> some multiple of X
(ie. they are parallel).

In machine learning, information is tangled in raw data. 
Intelligence is based on the ability to extract the principal components of information 
inside a stack of hay. Mathematically, eigenvalues and eigenvectors provide a way 
to identify them. Eigenvectors identify the components and eigenvalues quantify its
significance.

Marginalization:

       Marginalisation is a method that requires summing over the possible values of 
one variable to determine the marginal contribution of another.

Sometimes the method is called integrating out the nuisance variable.


https://towardsdatascience.com/probability-concepts-explained-marginalisation-2296846344fc#:~:text=Marginalisation%20tells%20us%20to%20just,properties%20we%20want%20(inference).

Binomial and Multinomial distribution:
       These distributions rely on the outcome of events. If the outcome is 2 then binomial
else if its multiple then it's multinomial.

In probability theory, the multinomial distribution is a 
generalization of the binomial distribution. For Example,
You roll a die ten times to see what number you roll. 
There are 6 possibilities (1, 2, 3, 4, 5, 6), so this is a multinomial experiment.
If you rolled the die ten times to see how many times you roll a three, 
that would be a binomial experiment (3 = success, 1, 2, 4, 5, 6 = failure)

Degrees of Freedom:


Student-T Distribution(Cauchy Distribution):

       t-distribution is also similar to Gaussian or normal distribution which is also symmetrical and
bell shaped but with heavier tail than the normal distribution.(ie.) more values in the distribution
are located in the tail than at the center compared to gaussian distribution.

This distribution is used for confidence interval estimation for population.It assumes that the 
underlying population is normally distributed.Beyond 30 degrees of freedom, the t-distribution
and the normal distribution become so similar

In statistical terms we use a metric called kurtosis to measure how "heavy-tailed" a distribution is.
Thus, kurtosis of t-distribution is greater than the normal distribution.

In practice, we use the t-distribution most often when performing hypothesis tests or constructing 
confidence intervals.  


Univariate Gaussian Distribution:

     the normal distribution also known as Gaussian Distr is defined by two parameters:
	 1. Mean(mu) - controls the Gaussian Ceter's position.
	 2. Standard Deviation(sigma) - controls the shape of the distribution.
	 
	def univariate_normal(x, mean, variance):
		return ((1. / np.sqrt(2 * np.pi * variance)) *
									np.exp(-(x - mean)**2 / (2 * variance)))
									
Multivariate Gaussian Distribution:

	it is a multidimensional Generalization of the one dimentional normal distribution.
it represents the distribution of multivariate random variable, that is made up of 
multiple random variables which can be correlated with each other.

	Like the univariate normal distribution, the multivariate normal is defined by sets of
parameters: 
	1. the mean vector μ. which is expected value of the distribution 
	2. the variance-covariance matrix Σ, which measures how two random variables
       depend on each other and how they change together,
	   We denote the covariance between variables X and Y as Cov(X,Y).
	 
	 If x1 and x2 is independent, covariance between x1 and x2 is set to zero.
	 If x1 and x2 is set to be different than 0, we can say that both variable 
	 are correlated.
	   
	def multivariate_normal(x, d, mean, covariance):
		x_m = x - mean
		return (1. / (np.sqrt((2 * np.pi)**d * np.linalg.det(covariance))) * 
            np.exp(-(np.linalg.solve(covariance, x_m).T.dot(x_m)) / 2))
			
		
where x is a random vector of size d, μ is d×1 mean vector. 
Σ is the (symmetric and positive definite) covariance matrix of size d×d. 
|Σ| is the determinant.We denote this multivariate normal distribution as N(μ,Σ).

1st plot - circular or spherical
2nd plot - centered eliptical 
3rd plot - left aligned eliptical due to negative value in non diagonal elements.

The first plot is refered to as a Spherical Gaussian, since the probability distribution has spherical (circular) symmetry. The covariance matrix is a diagonal covariance with equal elements along the diagonal. By specifying a diagonal covariance, what we’re seeing is that there’s no correlation between our two random variables, because the off-diagonal correlations takes the value of 0. You can simply interpret it as there is no linear relationship exists between variables. However, note that this does not necessarily mean that they are independent. Furthermore, by having equal values of the variances along the diagonal, we end up with a circular shape to the distribution because we are saying that the spread along each one of these two dimensions is exactly the same.

In contrast, the middle plot’s covariance matrix is also a diagonal one, but we can see that if we were to specify different variances along the diagonal, then the spread in each of these dimensions is different and so what we end up with are these axis-aligned ellipses. This is refered to as a Diagonal Gaussian.

Finally, we have the full Gaussian distribution. A full covariance matrix allows for correlation between the two random variables (non-zero off-diagonal value) we can provide these non-axis aligned ellipses. So in this example that we’re showing here, these two variables are negatively correlated, meaning if one variable is high, it’s more likely that the other value is low.


Gaussian Mixture Model:

Probability of MultiVariate Gaussian Distribution:
			j - the jth number of the gaussian distribution from total gaussians available.
			i - the ith number of the data sample from total data samples available.
			
			
		N(xi,mu(j),sigma(j)) = 1/(((2*pi)**(D/2))*(sigma(j)**1/2)) * 
		(PDF of a Gaussian)									exp(-(1/2)*((xi-mu(j))trans)*(sigma(j)-1)*(xi-mu(j)))

Estimation Using EM Algorithm:
	1. do E step - to find the likelihood or probability based on previous estimated
parameters
	2. do M step - to find the distribution parameters like Mean,Covariance,Variance
based on the likelihood calculated.

Hidden Markov Models:

Markov chain or Markov model - special type of discrete stochastic process in which
probability of an event occuring only depend on the immediate previous event.

Markov chains are generally defined by a set of states and the transition probabilities 
between each state. -> "Future is independent of the past given the present."

Markov Process - stochastic process in which the distribution of future event only depend 
on the present state or event.
Markov Chain - any chain that follows the markov property is called the markov chain.
Markov Property - The future state depend only on current state and independent of past
state. p(x[t+1]|x[t]).

   State:       Probability:  
1. hidden        Transition
2. observable    Emission 
3.               Initial or Prior.

Transition Probability Matrix(A) -  Markov chains are generally defined by a set of states 
and transition probabilities between each state.

The transition probabilities from one state to another are
generally represented in the form of a matrix known as Transition Matrix. also known
as Markov Matrix.

Initial Probability Vector(Pi)- Initial probability of the hidden state.  

Emission Probabilities - These define the probability of seeing a certain observed state
given a certain probability value for the hidden variables.

There are three fundamental problems for HMMs:

Given the model parameters and observed data, estimate the optimal sequence
of hidden states.

Given the model parameters and observed data, calculate the model likelihood.

Given just the observed data, estimate the model parameters.

The first and the second problem can be solved by the dynamic programming algorithms
known as the Viterbi algorithm and the Forward-Backward algorithm, respectively. 
The last one can be solved by an iterative Expectation-Maximization (EM) algorithm,
known as the Baum-Welch algorithm.  

Maximum Likelihood Estimation:

In Maximum Likelihood Estimation, we wish to maximize the probability of observing
the data from the joint probability distribution given a specific probability 
distribution and its parameters, stated formally as:

P(X,theta)  or  P(x1, x2, x3, …, xn ; theta)

This resulting conditional probability is referred to as the likelihood of observing 
the data given the model parameters.

The objective of Maximum Likelihood Estimation is to find the set of parameters (theta)
that maximize the likelihood function, e.g. result in the largest likelihood value.

maximize P(X,theta)

Maximum a Posteriori (MAP):

Recall that the Bayes theorem provides a principled way of calculating a 
conditional probability.

It involves calculating the conditional probability of one outcome 
given another outcome, using the inverse of this relationship, stated as follows:

P(A | B) = (P(B | A) * P(A)) / P(B)

The quantity that we are calculating is typically referred to as the posterior
probability of A given B. P(A) is referred to as the prior probability of A.

The normalizing constant of P(B) can be removed,
the posterior can be shown to be proportional to the probability of B given A
multiplied by the prior.

P(A | B) is proportional to P(B | A) * P(A)
Or, simply:
P(A | B) = P(B | A) * P(A)

This is a helpful simplification as we are not interested in estimating a probability,
but instead in optimizing a quantity. A proportional quantity is good enough 
for this purpose.

We can now relate this calculation to our desire to estimate a distribution 
and parameters (theta) that best explains our dataset (X), as we described 
in the previous section. This can be stated as:

P(theta | X) = P(X | theta) * P(theta)

Maximizing this quantity over a range of theta solves an optimization problem
for estimating the central tendency of the posterior probability
(e.g. the model of the distribution). As such, this technique is referred 
to as “maximum a posteriori estimation,” or MAP estimation for short.

maximize P(X | theta) * P(theta)

We are typically not calculating the full posterior probability distribution,
and in fact, this may not be tractable for many problems of interest.


Rejection Sampling:
    In this method we take a easier to handle distribution ie. something which we have
known and this distribution size should be bigger than our unknown distribution. After
rejecting the samples which are from the unknown distribution we can determine the 
parameter of the unknown distribution.
    eg: Uniform and Gaussian are easier to sample from.
	P(x) = 1/Z * cos(x) if -pi/2<=x<=+pi/2
	           0       otherwise.
			   
	what's the mean and standard deviation of this distribution function?
if we draw a gaussian distribution over this distribution. it's greater than the cosine
function distribution and easier to sample from. And after rejecting the points/samples
from the cosine distribution, we can able to determine the parameters mean & std. dev
for that distribution.

This works for some simpler distributions and also depending on when we're able to come
up with a nice proposed distribution.

Markov Chain Monte Carlo: 

	Probabilistic Inference involves estimating expected value or density using a
probabilistic model. Often, directly inferring values is not tractable with 
probabilistic models,and instead, approximation methods must be used.

    Monte Carlo is a technique for randomly sampling a probability distribution and 
approximating a desired quantity.  From the samples that are drawn, we can then 
estimate the sum or integral quantity as the mean or variance of the drawn samples.
The problem with this is high dimensionality of probability distribution and the
assumption of samples drawn from target distribution to be independant.

    Markov chain Monte carlo provides class of algorithms for systematic random sampling
from high dimensional probability distributions. Unlike Monte Carlo methods that draws
independant samples from distribution,Markov Chain Monte Carlo methods draw samples where
next sample is dependent on the existing sample by the markov chain property. This allows
the algorithms to narrow down in on the quantity that is being approximated from the 
distribution.The idea is that the chain will settle on (find equilibrium) on the
desired quantity we are inferring

The most common general Markov Chain Monte Carlo algorithm is called Gibbs Sampling; 
a more general version of this sampler is called the Metropolis-Hastings algorithm.
   
Gibbs Sampling:

   It's one of the markov chain monte carlo sampling methods. If we have a good sampled 
distribution from the unknown distribution then we can estimate the original distribution.
It breaks the target distribution into pieces so that we can easily sample in a piece.

--------------------------------------------------------------------------------------------------------------------------------

Drichlet Distribution:

This is a probability distribution as well - but it is not sampling from the space of 
real numbers. Instead it is sampling over a probability simplex.

And what is a probability simplex? It’s a bunch of numbers that add up to 1.
For example:

(0.6, 0.4)
(0.1, 0.1, 0.8)
(0.05, 0.2, 0.15, 0.1, 0.3, 0.2)

These numbers represent probabilities over K distinct categories. In the above examples,
K is 2, 3, and 6 respectively.

Accuracy Metrics in Regression:

unlike classification, accuracy in a regression model is slightly harder to illustrate. 
It is impossible for you to predict the exact value but rather how close your prediction is against
the real value.

There are 3 main metrics for model evaluation in regression:
1. R Square/Adjusted R Square

2. Mean Square Error(MSE)/Root Mean Square Error(RMSE)

3. Mean Absolute Error(MAE)

1. R Square/Adjusted R Square:
         R square measures how much variablity in the dependant variable(y) is explained by the model.
it is the square of the correlation coefficient(R).

R Square is calculated by the sum of squared of prediction error divided by the total sum of the square
which replaces the calculated prediction with mean.R Square value is between 0 to 1 and a bigger value
indicates a better fit between prediction and actual value.
		  SS(res) = Sum of Squares of residuals
		  SS(reg) = Sum of Squares of regression
		  SS(tot) = SS(res) + SS(reg) this measures the total variation in the dependant variable.
		  
		 (R squared) =  1 - SS(res)/SS(tot) = 1 - sum((y_label - y_hat)2)/sum((y_label - y_mean)2)

However, it does not take into consideration of overfitting problem. If your regression model has many
independent variables, because the model is too complicated, it may fit very well to the training data 
but performs badly for testing data. That is why Adjusted R Square is introduced because it penalizes you
for adding independent variable that do not help in predicting the dependent variable and adjust the 
metric to prevent overfitting issues. the diff between r squared and adj r squared is based on degrees of
freedom.

          adj(R squared) = 1 - [((1-(R squared))(n-1))/n-k-1]

dft is the degrees of freedom n– 1 of the estimate of the population variance of the dependent variable,
and dfe is the degrees of freedom n – p – 1 of the estimate of the underlying population error variance.		  

2. Mean Squared Error(MSE):

While R Square is a relative measure of how well the model fits dependent variables, Mean Square Error 
is an absolute measure of the goodness for the fit.


		  MSE = (1/N)*sum((y_label - y_hat)2)
MSE is calculated by the sum of square of prediction error which is real output minus predicted output 
and then divide by the number of data points.It gives you an absolute number on how much your predicted
results deviate from the actual number.

3. Mean Absolute Error(MAE):
	 Mean Absolute Error(MAE) is similar to Mean Square Error(MSE). 
However, instead of the sum of square of error in MSE, MAE is taking the sum of the absolute value 
of error.
		MAE = (1/N)*sum(abs(y_label - y_hat))

-----------------------------------------------------------------------------------------------------------------------------------
Probabilistic Latent Semantic Analysis:

PLSA or Probabilistic Latent Semantic Analysis is a technique used to model topic 
information to documents under a probabilistic framework.Latent because the topics
are treated as latent or hidden variables.

In this parametrization, we sample a document first then based on the document
we sample a topic, and based on the topic we sample a word, which means d and w
are conditionally independent given a hidden topic ‘z’.

Let’s formally define the variables that appear in PLSA.

         1. Document - D={d1,d2,d3,...,d(N)},N is the no of documents. 
		 2. Words - W={w1,w2,w3,...,w(M)},M is the size of our vocabulary.
		 3. Topics - Z={z1,z2,z3,...,z(K)},K is the no of topics which is defined by us.
This is the Latent or Hidden Variable.

Basic Assumption for Topic Modelling:

	1. Each document consists of a mixture of topics, and
	2. Each topic consists of a collection of words.
	

Given a document d, a topic z is present in that selected document with probability P(z|d)
Given a topic z, word w is drawn from the topic z with probability P(w|z)
Here we associate z with (d,w) and described a generative process where we select
a document, then a topic, and finally a word from that topic. 

	1. We select a document from the corpus with a probability P(d)

	2. For every word in the selected document dn, and word wi

Select a topic zi from a conditional distribution with a probability P(z|dn).
Select a word with a probability P(w|zi).

There are two main assumptions in PLSI:
    1. In text vectorization techniques, the word order doesn't matter.
	2. Words and documents are conditionally independant. (ie.) P(w,d|z) = P(z|d)*P(w|z)
	
Parmeterization-1:
	 We start with P(d). then P(d,w),
     P(d,w) = P(w|d)*P(d)
	 P(w|d) = P(w,z|d) = Sum for all z{P(w|d,z)*P(z|d)}
	 P(w|d) = P(w|z)*P(z|d) By conditional independence assumption.
	 
	 P(d,w) = P(w|d)*P(d) = P(d)*Sum for all z{P(z|d)*P(w|z)}.
	 
The right-hand side of the above equation tells us that how likely it is to observe
some document and then based upon the distribution of topics in that document, how 
likely it is to find a certain word within that document. This is the exact 
interpretation of that component in the equation.
	 
	 For P(w|z) - (M-1)*K parameters to determine. P(z|d) - (K-1)*N parameters to 
determine.

Parmeterization-2:
	 We start with P(z).then,
	 P(d,w) = P(z)*P(d|z)*P(w|z). this can be equivalently modelled as,
	 A~ U(t)*S(t)*V(t) - Singular value Decomposition.
	 P(z) corresponds to singular Value of S(t).
	 P(d|z) corresponds to document-topic matrix U(t).
	 P(w|z) corresponds to Term-topic matrix V(t).
	 
Both these parameters are modelled as multinomial distributions. they can be trained
using Expectation Maximization Algorithm.

EM is a method of finding the likeliest parameter estimates for a model 
which depends on unobserved, latent/hidden variables.

EM algorithm has the following two steps:

Step-1: This step is known as the expectation (E) step, where posterior probabilities 
are computed for the latent variables,

Step-2: This step is known as the maximization (M) step, where parameters are updated
according to the likelihood function.
	 
It can be designed in two ways:
		Latent Variable Model
		Matrix Factorization

Matrix Factorization:

Consider a document-word matrix of dimensions N*M, where N is the number of documents
and M is the size of the vocabulary. The elements of the matrix are counts of the 
occurences of a word in a document.


Entropy in Information Theory:

	In information theory, the entropy of a random variable is the average level of "information", 
"surprise", or "uncertainty" inherent to the variable's possible outcomes.

	The information content, also called the surprisal or self-information, of an event E is a function 
which increases as the probability P(E) of an event decreases. When P(E) is close to 1, the surprisal
of the event is low, but if P(E) is close to 0, the surprisal of the event is high. This relationship
is described by the function:

	I(X) = log2(1/P(X)) = -log2(P(X))

KullBack-Leibler Divergence:
		
	In mathematical Statistics,KL Divergence also called the relative entropy is the statistical 
distance between how one Probability Distribution P is different from the second Distribution Q. 
Relative Entropy is a non-negative function of two distributions or measures. a relative entropy of
0 indicates that the two distributions in general are identical quantities of information.

		Dkl(P||Q) = Sum(P(X)*log(P(X)/Q(X))).
		
	In other words, it is the expectation of the logarithmic difference between the probabilities
P and Q, where the expectation is taken using the probabilities P. For continuous variables, the 
integration is taken over.

-------------------------------------------------------------------------------------------------------------------------------
	
L1 and L2 Regularization:

          The implementation of Gradient Descent has no regularization. Because of this our 
model will likely to be overfit the training data. when our task is a simple one but the model
we're using is complex. we need to use regularizers to overcome overfitting the training data.

L1 - L1 norm or Lasso regularization combats overfitting by shrinking parameters towards 0.
this makes some features obsolete.

A selection of the input features would have weights equal to zero,
and the rest would be non-zero. 

For example, imagine we want to predict housing prices using machine learning. 
Consider the following features: 

Street – road access,
Neighborhood – property location,
Accessibility – transport access,
Year Built – year the house was built in,
Rooms – number of rooms,
Kitchens – number of kitchens,
Fireplaces – number of fireplaces in the house. 

When predicting the value of a house, intuition tells us that different input features 
won’t have the same influence on the price.For example, it’s highly likely that the neighborhood
or the number of rooms have a higher influence on the price of the property than the number of fireplaces.

So, our L1 regularization technique would assign the fireplaces feature with a zero weight, 
because it doesn’t have a significant effect on the price.
We can expect the neighborhood and the number rooms to be assigned non-zero weights,
because these features influence the price of a property significantly. 

Mathematically, we express L1 regularization by extending our loss function like such: 

Essentially, when we use L1 regularization, we are penalizing the absolute value of the weights. 

In real world environments, we often have features that are highly correlated. 
For example, the year our home was built and the number of rooms in the home may have a high correlation.
Something to consider when using L1 regularization is that when we have highly correlated features, 
the L1 norm would select only 1 of the features from the group of correlated features in an arbitrary nature, 
which is something that we might not want.

Nonetheless, for our example regression problem,
Lasso regression (Linear Regression with L1 regularization) 
would produce a model that is highly interpretable, and only uses a subset of input features,
thus reducing the complexity of the model.  

L2 - L2 regularization, or the L2 norm, or Ridge (in regression problems), 
combats overfitting by forcing weights to be small, but not making them exactly 0. 


So, if we’re predicting house prices again, this means the less significant features 
for predicting the house price would still have some influence over the final prediction,
but it would only be a small influence. 

The regularization term that we add to the loss function when performing L2 regularization
is the sum of squares of all of the feature weights:


So, L2 regularization returns a non-sparse solution since the weights will be non-zero 
(although some may be close to 0).

A major snag to consider when using L2 regularization is that it’s not robust to outliers. 
The squared terms will blow up the differences in the error of the outliers. 
The regularization would then attempt to fix this by penalizing the weights. 

https://medium.com/analytics-vidhya/topic-modeling-with-latent-dirichlet-allocation-lda-196c287e221

jumedwar@indiana.edu


BIT DEPTH is determined by the number of bits used to define each pixel. The greater the bit depth,
the greater the number of tones (grayscale or color) that can be represented. Digital images may be 
produced in black and white (bitonal), grayscale, or color.

Bit Depth: Left to right - 1-bit bitonal(B&W), 8-bit grayscale, and 24-bit color images.

Binary calculations for the number of tones represented by common bit depths:

1 bit (21) = 2 tones
2 bits (22) = 4 tones
3 bits (23) = 8 tones
4 bits (24) = 16 tones
8 bits (28) = 256 tones
16 bits (216) = 65,536 tones
24 bits (224) = 16.7 million tones

Researchers From MIT and Cornell Develop STEGO (Self-Supervised Transformer With Energy-Based
Graph Optimization): A Novel AI Framework That Distills Unsupervised Features Into High-Quality
Discrete Semantic Labels
Quick Read: https://lnkd.in/ggsVt4kz

Paper: https://lnkd.in/gZyMXENK

Github: https://lnkd.in/gDwVascQ

-------------------------------------------------------------------------------------------------------------------
References:

https://ruihongqiu.github.io/posts/2021/06/nngp/
https://towardsdatascience.com/infinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf
https://rajatvd.github.io/NTK/

width of neural network - number of neurons in each layer corresponds to the width of each layer.

Depth of neural network - the number of layers present in the network including output and input layer
corresponds to the depth of the neural network. 


Neural Network Tangent:

		Sometimes neural networks become become black boxes after fitted to an classification or regression
problem. We cannot make sense of the final weights that have been learned or adequately visualize the 
problem space. 
	
	We will explore the background and application of neural networks at infinite width. Neural tangents
offers elegant solution by providing a framework to build these networks and from this we can gain more
insights from our model and can provide better analytics. 

Neural Network Primer:
	
	Generally, we build a neural network from assumptions about the problem space. Usually a network 
topology fits to solve a class of problems. We initialize a network weights from a standard normal 
distribution and then we train the model by adapting the model parameters(network weigths) to minimize
a loss function for the problem defined previously. After training we test on unseen data with some 
evaluation metrics. One Important Question is: Why deep neural networks genearlize so well eventhough
they tend to be overparameterized?.

What happens when we stretch DNN?
	
	When we increase the width of the neural network we can gain some insights and overall better 
understanding. Very wide networks show convergence to gaussian processes(a stochastic processs in
which every finite collection of random variables has a multivariate normal distribution such that
this joint distribution is a continuous distribution over function with a continuous time domain).
This is also applicable for deeper networks. What do we get from transforming a network into 
gaussian process?.
	
	This allows for better understanding and analysis over uncertainities in the network. we can also
perform bayesian inference over the prior and posterior and analyze them. One important aspect is that
the covariance of the Gaussian Process(GP) is assosiated with a kernel for its covariance function. 
These kernels allow for better analytical properties.
 
There are two kernels namely:
	1. Neural Network Gaussian Process.
		
		NNGP is simply defined as Kz = E(theta)[z(i),z(i).T]
		
		where z is the predefined network architecture with parameters,activations and pre-activations
like convolutions,pooling etc.,

	2. Neural Tangent Kernel.
		
		NTK is defined as Tz = E(theta)[Diff(z(i)/theta),Diff(z(i)/theta).T]
		
		In layman’s terms, the last kernel describes the change in the network parameters up until z.


Bias - Variance TradeOff:

Consider a common neural network task such as image recognition, and think over a neural network that recognizes the presence of pandas in a picture. 
We can confidently assess that a human can carry out this task with a near 0% error. As a consequence, this is a reasonable 
benchmark for the accuracy of the image recognition network.After training the neural network on the training set and 
evaluating its performances on both the training and validation sets, we may come up with these different results:

train error = 20% and validation error = 22%
train error = 1% and validation error = 15%
train error = 0.5% and validation error = 1%
train error = 20% and validation error = 30%

The first example is a typical instance of high bias: the error is large on both the train and validation sets. Conversely,
the second example suffers from high variance, having a much lower accuracy when dealing with data the model didn’t learn from.
The third result represents low variance and bias, the model can be considered valid. Finally, the fourth example shows a case
of both high bias and variance: not only the training error is large when compared with the benchmark, but the validation 
error is also higher.

L1 and L2 Regularization for Neural Networks:

Similar to classical regression algorithms (linear, logistic, polynomial, etc.), L1 and L2 regularizations find place also for
preventing overfitting in high variance neural networks. In order to keep this article short and on-point, I won’t recall how
L1 and L2 regularizations work on regression algorithms, but you can check this article for additional information.

The idea behind the L1 and L2 regularization techniques is to constrain the model’s weights to be smaller or to shrink some of
them to 0.

Consider the cost function J of a classical deep neural network:

			J(w1,b1,......,wL,bL)  = (1/m)*(Summ{1 to m}(L(y_hat,y)))
			
The cost function J is, of course a function of weights and biases of each layer 1 to L. m is number of training examples and
L is the loss function.

L1 Regularization:

	In L1 Regularization we add the following term to the cost function J:
			(lambda/2m)*Summ{1 to L}(||wL||1)
	
	where the matrix norm is the sum of the absolute value of the weights for each layer 1, …, L of the network:
			||wL||1 = Summ{i}{j}(w(L)ij)
	
	λ is the regularization term. It’s a hyperparameter that must be carefully tuned. λ directly controls the impact of the 
regularization: as λ increases, the effects on the weights shrinking are more severe.

The complete cost function under L1 Regularization becomes:

	J(w1,b1,.......,wL,bL) = (1/m)*(Summ{1 to m}(L(y_hat,y))) + (lambda/2m)*Summ{1 to L}(||wL||)
	
For λ=0, the effects of L1 Regularization are null. Instead, choosing a value of λ which is too big, will over-simplify the
model, probably resulting in an underfitting network.L1 Regularization can be considered as a sort of neuron selection process
because it would bring to zero the weights of some hidden neurons.
	
L2 Regularization
	
	In L2 Regularization, the term we add to the cost function is the following:
			
			(Lambda/2m)*Summ{1 to L}(||wL||2^2)
			
	In this case, the regularization term is the squared norm of the weights of each network’s layer. This matrix norm is called
Frobenius norm and, explicitly, it’s computed as follows:

			||wL||2^2 = Summ{i= 1 to L}{j = 1 to L-1}(wL)^2

	Please note that the weight matrix relative to layer l has n^{[l]} rows and n^{[l-1]} columns.Again, λ is the regularization
term and for λ=0 the effects of L2 Regularization are null.L2 Regularization brings towards zero the values of the weights, 
resulting in a more simple model.

How do L1 and L2 Regularization reduce overfitting?
	
	L1 and L2 Regularization techniques have positive effects on overfitting to the training data for two reasons:

the weights of some hidden units become closer (or equal) to 0. As a consequence, their effect is weakened and the resulting 
network is simpler because it’s closer to a smaller network. As stated in the introduction, a simpler network is less prone to
overfitting.

for smaller weights, also the input z of the activation function of a hidden neuron becomes smaller. For values close to 0, 
many activation functions behave linearly. The second reason is not trivial and deserves an expansion. Consider a hyperbolic 
tangent (tanh) activation function, whose graph is the following:
       
	From the function plot we can see that if the input value x is small, the function tanh(x) behaves almost linearly. 
When tanh is used as the activation function of a neural network’s hidden layers, the input value is:

		zL = wL*a(L-1)+bL
	which for small weights w is also close to zero.
	
	If every layer of the neural network is linear, we can prove that the whole network behaves linearly. Thus, constraining
some of the hidden units to mimic linear functions, leads to a simpler network and, as a consequence, helps to prevent 
overfitting.A simpler model often is not capable to capture the noise in the training data and therefore, overfitting is less
frequent.

Dropout:

	The idea of dropout regularization is to randomly remove some nodes in the network. Before the training process,
we set a probability (suppose p = 50%) for each node of the network. During the training phase, each node has a p probability
to be turned off. The dropout process is random, and it is performed separately for each training example. As a consequence,
each training example might be trained on a different network.

	As for L2 Regularization, the result of dropout regularization is a simpler network, and a simpler network leads to a less
complex model.

Consider the network illustrated above, and focus on the first unit of the second layer. Because some of its inputs may be 
temporarily shut down due to dropout, the unit can’t always rely on them during the training phase.As a consequence, the hidden 
unit is encouraged to spread its weights across its inputs. Spreading the weights has the effect of decreasing the squared norm
of the weight matrix, resulting in a sort of L2 regularization.

Setting the keeping probability is a fundamental step for an effective dropout regularization. Typically, the keeping probability
is set separately for each layer of the neural network. For layers with a large weight matrix, we usually set a smaller keeping
probability because, at each step, we want to conserve proportionally fewer weights with respect to smaller layers.

Other Regularization Techniques:
	
	In addition to L1/L2 regularization and dropout, there exist other regularization techniques. Two of them are data augmentation and early stopping.

From the theory, we know that training a network on more data has positive effects on reducing high variance. As getting more 
data is often a demanding task, data augmentation is a technique that, for some applications, allows machine learning 
practitioners to get more data almost for free.In computer vision, data augmentation provides larger training set by flipping,
zooming, and translating the original images.In the case of digits recognition, we can also impose distortion on the images.

Early stopping, as the name suggests, involves stopping the training phase before the initially defined number of iterations. 
If we plot the cost function on both the training set and the validation set as a function of the iterations, we can experience
that, for an overfitting model, the training error always keeps decreasing but the validation error might start to increase 
after a certain number of iterations. When the validation error stops decreasing, that is exactly the time to stop the training
process. By stopping the training process earlier, we force the model to be simpler, thus reducing overfitting.

In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter 
estimated across samples can be reduced by increasing the bias in the estimated parameters. The bias–variance dilemma or 
bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised
learning algorithms from generalizing beyond their training set:[1][2]

The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the
relevant relations between features and target outputs (underfitting).

The variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm
modeling the random noise in the training data (overfitting).The bias–variance decomposition is a way of analyzing a learning 
algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, 
and a quantity called the irreducible error, resulting from noise in the problem itself.

---------------------------------------------------------------------------------------------------------------------------------

Kernel Definition:

Briefly speaking, a kernel is a shortcut that helps us do certain calculation faster which otherwise would involve computations
in higher dimensional space.

Mathematical definition: K(x, y) = <f(x), f(y)>. Here K is the kernel function, x, y are n dimensional inputs. f is a map from
n-dimension to m-dimension space. < x,y> denotes the dot product. usually m is much larger than n.

Intuition: normally calculating <f(x), f(y)> requires us to calculate f(x), f(y) first, and then do the dot product. These two 
computation steps can be quite expensive as they involve manipulations in m dimensional space,
where m can be a large number. But after all the trouble of going to the high dimensional space, the result of the dot product
is really a scalar: we come back to one-dimensional space again! Now, the question we have is: do we really need to go through 
all the trouble to get this one number? do we really have to go to the m-dimensional space? The answer is no, if you find a 
clever kernel.

Simple Example: x = (x1, x2, x3); y = (y1, y2, y3). Then for the function f(x) = (x1x1, x1x2, x1x3, x2x1, x2x2, x2x3, x3x1, x3x2, x3x3), the kernel is K(x, y ) = (<x, y>)^2.

Let's plug in some numbers to make this more intuitive: suppose x = (1, 2, 3); y = (4, 5, 6). Then:
f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)
f(y) = (16, 20, 24, 20, 25, 30, 24, 30, 36)
<f(x), f(y)> = 16 + 40 + 72 + 40 + 100+ 180 + 72 + 180 + 324 = 1024

A lot of algebra. Mainly because f is a mapping from 3-dimensional to 9 dimensional space.

Now let us use the kernel instead:
K(x, y) = (4 + 10 + 18 ) ^2 = 32^2 = 1024
Same result, but this calculation is so much easier.

Additional beauty of Kernel: kernels allow us to do stuff in infinite dimensions! Sometimes going to higher dimension is not 
just computationally expensive, but also impossible. f(x) can be a mapping from n dimension to infinite dimension which we may 
have little idea of how to deal with. Then kernel gives us a wonderful shortcut.

Relation to SVM: now how is related to SVM? The idea of SVM is that y = w phi(x) +b, where w is the weight, phi is the feature 
vector, and b is the bias. if y> 0, then we classify datum to class 1, else to class 0. We want to find a set of weight and 
bias such that the margin is maximized. Previous answers mention that kernel makes data linearly separable for SVM. I think a 
more precise way to put this is, kernels do not make the data linearly separable. The feature vector phi(x) makes the data 
linearly separable. Kernel is to make the calculation process faster and easier, especially when the feature vector phi is of 
very high dimension (for example, x1, x2, x3, ..., x_D^n, x1^2, x2^2, ...., x_D^2).

Why it can also be understood as a measure of similarity:
if we put the definition of kernel above, <f(x), f(y)>, in the context of SVM and feature vectors, it becomes <phi(x), phi(y)>.
The inner product means the projection of phi(x) onto phi(y). or colloquially, how much overlap do x and y have in their 
feature space. In other words, how similar they are.

The kernel has two properties:
It is symmetric in nature k(xn , xm) = k(xm , xn).
It is Positive semi-definite.

--------------------------------------------------------------------------------------------------------------------------------

Support Vector Machine:

	The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of 
features) that distinctly classifies the data points.

Margin

	Margin represents the shortest perpendicular distance between the decision boundary and the data points.
	You find the hyperplane that maximizes the margin.By the way, those closest data samples are support vectors

	Which one do you prefer?
		You prefer the one with the larger margin z1<z2.
	
SVM Mathematics:	
https://www.analyticsvidhya.com/blog/2020/10/the-mathematics-behind-svm/#:~:text=A%20Support%20Vector%20Machine%20or,to%20as%20Support%20Vector%20Classification.



-------------------------------------------------------------------------------------------------------------------------------

Bernoulli Distribution:

A Bernoulli event is one for which the probability the event occurs is p and the probability the event does not occur is 1-p;
i.e., the event is has two possible outcomes (usually viewed as success or failure) occurring with probability p and 1-p, 
respectively.  A Bernoulli trial is an instantiation of a Bernoulli event.  So long as the probability of success or failure
remains the same from trial to trial (i.e., each trial is independent of the others), a sequence of Bernoulli trials is called
a Bernoulli process

Binomial Distribution:

The Bernoulli distribution represents the success or failure of a single Bernoulli trial.  The Binomial Distribution represents
the number of successes and failures in n independent Bernoulli trials for some given value of n.  For example, if a 
manufactured item is defective with probability p, then the binomial distribution represents the number of successes and 
failures in a lot of n items.  In particular, sampling from this distribution gives a count of the number of defective items 
in a sample lot.Another example is the number of heads obtained in tossing a coin n times

Poisson Distribution:

the Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events 
occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the
time since the last event.

	f(k,lambda) = (lambda)^k * exp^-(lambda)/k!
	
Eg: For instance, a call center receives an average of 180 calls per hour, 24 hours a day. The calls are independent; receiving
one does not change the probability of when the next one will arrive. The number of calls received during any minute has a 
Poisson probability distribution with mean 3: the most likely numbers are 2 and 3 but 1 and 4 are also 
likely and there is a small probability of it being as low as zero and a very small probability it could be 10.
	
Precision Matrix:

	In statistics, the precision matrix or concentration matrix is the matrix inverse of the covariance matrix or dispersion 
matrix, P=C-1.[1][2][3] For univariate distributions, the precision matrix degenerates into a scalar precision, defined as the
reciprocal of the variance, p=1/σ2.


Gamma Distribution:

	In probability theory and statistics, the gamma distribution is a two-parameter family of continuous probability distributions.
The exponential distribution, Erlang distribution, and chi-square distribution are special cases of the gamma distribution. 
There are two different parameterizations in common use:

	With a shape parameter k and a scale parameter θ.
	With a shape parameter α = k and an inverse scale parameter β = 1/θ, called a rate parameter.

In each of these forms, both parameters are positive real numbers.

	The gamma distribution is the maximum entropy probability distribution (both with respect to a uniform base measure and with
respect to a 1/x base measure) for a random variable X for which E[X] = kθ = α/β is fixed and greater than zero, and 
E[ln(X)] = ψ(k) + ln(θ) = ψ(α) − ln(β) is fixed

Gaussian-Gamma Distribution:

	In probability theory and statistics, the normal-gamma distribution (or Gaussian-gamma distribution) is a bivariate four-
parameter family of continuous probability distributions.It is the conjugate prior of a normal distribution with unknown mean 
and precision.	
	
-------------------------------------------------------------------------------------------------------------------------------

Density Estimation:

	Density Estimation is the problem of estimating the probability distribution given the observation samples. Typically, 
estimating the entire distribution is intractable and we are fine with the expected value of the distribution such as 
mean or mode.

	There are many techniques for solving this problem, although two common approaches are:

1. Maximum a Posteriori (MAP), a Bayesian method.
2. Maximum Likelihood Estimation (MLE), a frequentist method

	Both approaches frame the problem as optimization and involve searching for a distribution and set of parameters
for the distribution that best describes the observed data.



Maximum Likelihood Estimation(MLE):

	In Maximum Likelihood Estimation, we wish to maximize the probability of observing the data from the joint probability 
distribution given a specific probability distribution and its parameters, stated formally as:

	P(X ; theta)
or

	P(x1, x2, x3, …, xn ; theta)

This resulting conditional probability is referred to as the likelihood of observing the data given the model parameters.

The objective of Maximum Likelihood Estimation is to find the set of parameters (theta) that maximize the likelihood function, 
e.g. result in the largest likelihood value.

Maximum A-Posteriori (MAP) Estimation:

	Bayes Theoram:   P(A | B) = (P(B | A) * P(A)) / P(B)
	
The quantity that we are calculating is typically referred to as the posterior probability of A given B and P(A) 
is referred to as the prior probability of A.The normalizing constant of P(B) can be removed, and the posterior can be 
shown to be proportional to the probability of B given A multiplied by the prior.

	P(A | B) is proportional to P(B | A) * P(A)
	
We can now relate this calculation to our desire to estimate a distribution and parameters (theta) that best explains our 
dataset (X), as we described in the previous section. This can be stated as:

	P(theta | X) = P(X | theta) * P(theta)

Maximizing this quantity over a range of theta solves an optimization problem for estimating the central tendency of the 
posterior probability (e.g. the model of the distribution). As such, this technique is referred to as 
“maximum a posteriori estimation,” or MAP estimation for short, and sometimes simply “maximum posterior estimation.”

	maximize P(X | theta) * P(theta)
	
We are typically not calculating the full posterior probability distribution, and in fact, this may not be tractable
for many problems of interest.Instead, we are calculating a point estimation such as a moment of the distribution, like the mode,
the most common value, which is the same as the mean for the normal distribution.


In fact, if we assume that all values of theta are equally likely because we don’t have any prior information (e.g. a uniform 
prior),then both calculations are equivalent.Because of this equivalence, both MLE and MAP often converge to the same optimization
problem for many machine learning algorithms. This is not always the case; if the calculation of the MLE and MAP optimization 
problem differ, the MLE and MAP solution found for an algorithm may also differ.

RelationShip between MAP and Machine Learning:
	
We can make the relationship between MAP and machine learning clearer by re-framing the optimization problem as being performed 
over candidate modeling hypotheses (h in H) instead of the more abstract distribution and parameters (theta); for example:

	maximize P(X | h) * P(h)

Here, we can see that we want a model or hypothesis (h) that best explains the observed training dataset (X) and that the prior
(P(h)) is our belief about how useful a hypothesis is expected to be, generally, regardless of the training data. The 
optimization problem involves estimating the posterior probability for each candidate hypothesis.

---------------------------------------------------------------------------------------------------------------------------------
Chain Rule of Probability:


	The chain rule for two random events A and B says:
		
		P(A Intersection B) = P(B|A)*P(A)
		
	P(A Intersection B Intersection C Intersection D) = P(A| B Intersection C Intersection D)*P(B Intersection C Intersection D)
				
				= P(A | B Intersection C Intersection D)*P(B | C Intersection D)* P(C | D) * P(D)


---------------------------------------------------------------------------------------------------------------------------------

Conjugate Prior:

	In Bayesian probability theory, if the posterior distribution p(θ | x) is in the same probability distribution family as the
prior probability distribution p(θ), the prior and posterior are then called conjugate distributions, and the prior is called a
conjugate prior for the likelihood function p(x | θ).

Prior vs Posterior Predictive Distribution:

	The prior predictive distribution, in a Bayesian context, is the distribution of a data point marginalized over its prior 
distribution. x~F(x~|theta) and theta~G(theta|alpha). That is, if , then the prior predictive distribution is the corresponding
distribution H(x~|alpha), where

	Ph(x~|alpha) = Integral(theta){Pf(x~|theta)Pg(theta|alpha)}.

This is similar to the posterior predictive distribution except that the marginalization (or equivalently, expectation) is taken
with respect to the prior distribution instead of the posterior distribution.

Furthermore, if the prior distribution G(theta|alpha) is a conjugate prior, then the posterior predictive distribution will 
belong to the same family of distributions as the prior predictive distribution. This is easy to see. If the prior distribution
  is conjugate, then

i.e. the posterior distribution also belongs to G(theta|alpha), but simply with a different parameter alpha' 
instead of the original parameter alpha. Then,

	P(x~|X,alpha) = Integral(theta){Pf(x~|theta)P(theta|X,alpha)}
				  = Integral(theta){Pf(x~|theta)P(theta|alpha')}
				  = Ph(x~|alpha)

Hence, the posterior predictive distribution follows the same distribution H as the prior predictive distribution, but with the
posterior values of the hyperparameters substituted for the prior ones.

-------------------------------------------------------------------------------------------------------------------------------
Marginal Likelihood:

	In Bayes theorem of a parameter θ with data D, we have:

	P(θ|D)=P(D|θ)P(θ)/P(D)

where P(D) as the marginal likelihood. Also, P(D) is the model evidence, unfortunately "model" is often dropped. The model 
evidence is also referred to as marginal likelihood.

	A marginal likelihood is a likelihood function that has been integrated over the parameter space. In Bayesian statistics,
it represents the probability of generating the observed sample from a prior and is therefore often referred to as model 
evidence or simply evidence.


Given a set of independent identically distributed data points X={x1,x2,x3...,xn} where xi~p(x|theta) according to some 
probability distribution parameterized by theta , where theta itself is a random variable described by a distribution, i.e.
theta ~ p(theta|alpha) the marginal likelihood in general asks what the probability p(X|alpha) is, where 
theta has been marginalized out (integrated out):

	p(X|alpha) = Integral(theta){p(X|theta)p(theta|alpha)}

The above definition is phrased in the context of Bayesian statistics. In classical (frequentist) statistics, the concept of
marginal likelihood occurs instead in the context of a joint parameter theta=(sci,lambda), where sci is the actual parameter
of interest, and lambda is a non-interesting nuisance parameter. If there exists a probability distribution for lambda,
it is often desirable to consider the likelihood function only in terms of sci, by marginalizing out Lambda:

	Lamda(sci,X) = p(X|sci) = Integral(lamda){P(X|lamda,sci)P(lamda|sci)}

Unfortunately, marginal likelihoods are generally difficult to compute. Exact solutions are known for a small class of 
distributions, particularly when the marginalized-out parameter is the conjugate prior of the distribution of the data.
In other cases, some kind of numerical integration method is needed, either a general method such as Gaussian integration or a 
Monte Carlo method, or a method specialized to statistical problems such as the Laplace approximation, Gibbs/Metropolis 
sampling, or the EM algorithm.

It is also possible to apply the above considerations to a single random variable (data point) x, rather than a set of 
observations. In a Bayesian context, this is equivalent to the prior predictive distribution of a data point.

-------------------------------------------------------------------------------------------------------------------------------
Model Evidence:

	p(y|m)=∫p(y,θ|m)dθ
		  =∫p(y|θ,m)p(θ|m)dθ
		  
the classical rule is P(A,B)=P(A|B)P(B), but it can also be applied to conditional probabilities like P(.|C) instead of P(.).
It then becomes
	
	P(A,B|C)=P(A|B,C)P(B|C)

(you just add a condition on C, but otherwise that's the same formula). You can then apply this formula for A=y, B=θ, and C=m.

You know from the law of total probability that, if {Bn} is a partition of the sample space, we obtain

	p(A)=∑np(A,Bn)

or, using the first formula:

	p(A)=∑np(A|Bn)p(Bn)

This easily extends to continuous random variables, by replacing the sum by an integral:

p(A)=∫p(A|B)p(B)dB

The action of making B "disappear" from p(A,B) by integrating it over B is called "marginalizing" (B has been marginalized out).
Once again, you can apply this formula for A=y, B=θ, and C=m.

m is the model. Your data y can have been generated from a certain model m, and this model itself has some parameters θ.
In this setting, p(y|θ,m) is the probability to have data y from model m parametrized with θ, and p(θ|m) is the prior distribution
of the parameters of model m.For example, imagine you are trying to fit some data using either a straight line or a parabola.
Your 2 models are thus m2, where data are explained as y=a2x2+a1x+a0+ϵ (ϵ is just some random noise) and its parameters are 
θ2=[a2 a1 a0] ; and m1, where data are explained as y=a1x+a0+ϵ and its parameters are θ1=[a1 a0].
	
-----------------------------------------------------------------------------------------------------------------------------------

Negative Log Likelihood:	
	It’s a cost function that is used as loss for machine learning models, telling us how bad it’s performing, the lower the
better.

	Negative: obviously means multiplying by -1. What? The loss of our model. Most machine learning frameworks only have
minimization optimizations, but we want to maximize the probability of choosing the correct category.

We can maximize by minimizing the negative log likelihood, there you have it, we want somehow to maximize by minimizing.

Likelihood:

	Likelihood refers to the chances of some calculated parameters producing some known data.
	
That makes sense as in machine learning we are interested in obtaining some parameters to match the pattern inherent to the data,
the data is fixed, the parameters aren’t during training.

Typically a model will output a set of probabilities (like [0.1, 0.3, 0.5, 0.1]), how does it relates with the likelihood? We 
are using NLL as the loss and the model outputs probabilities, but we said they mean something different.

how do they play together? Well, to calculate the likelihood we have to use the probabilities. To continue with
the example above, imagine for some input we got the following probabilities: [0.1, 0.3, 0.5, 0.1], 4 possible classes. If the
true answer would be the forth class, as a vector [0, 0, 0, 1], the likelihood of the current state of the model producing the
input is:

0*0.3 + 0*0.1 + 0*0.5 + 1*0.1 = 0.1.

NLL: -ln(0.1) = 2.3

Instead, if the correct category would have been the third class [0, 0, 1, 0]:

0*0.3 + 0*0.1 + 1*0.5 + 0*0.1 = 0.5

NLL: -ln(0.5) = 0.69

Take a breath and look at the values obtained by using the logarithm and multiplying by -1.

You see? The better the prediction the lower the NLL loss, exactly what we want! And same way works for other losses, the better
the output, the lower the loss.
	
(When do we use it?): we use it for the classification problems and the output of model is a probability distribution.	


---------------------------------------------------------------------------------------------------------------------------------

Cholesky decomposition for Matrix Inversion

	Matrix inversion is a classical problem, and can be very complicated for large matrices. There are many ways to simplify
this for special types of matrices. Among them, one is to transform the matrix into a set of upper or lower triangular matrices. 
Consider our target matrix A which is Hermitian and positive-definite. Such matrices are quite famous and an example
is the covariance matrix in statistics. It’s inverse is seen in the Gaussian probability density function for vectors. Then, 
Cholesky decomposition breaks

	A = L*L^T = U^T*U

where L is a lower triangular matrix, while U is an upper triangular matrix.

It is much easier to compute the inverse of a triangular matrix and there exist numerical solutions. Then the original matrix 
inverse is computed simply by multiplying the two inverses as

	A-1 = (L-1)^T*(L-1) = (U-1)*(U-1)^T

As bonus, the determinant is also much easier to compute.

	det(A) = det(A^2)

One can also use complex matrices, and just use a conjugate-transpose instead of transpose alone.

----------------------------------------------------------------------------------------------------------------------------------

what is the difference between XGBoost and LightGBM? In this post, we'll compare both algorithms and see which one might be the
best option for your problem.

Overall, both tools are excellent for creating accurate machine learning models. However, there are some key differences you should keep 
in mind:

Speed: LightGBM is known to be faster than XGBoost. This is because it uses subsampling techniques to reduce the amount of data it
needs to process.

Memory usage: XGBoost tends to use more memory than LightGBM. This can be a problem if you have a very large dataset.

Accuracy: In terms of accuracy, both tools are similar. However, in some cases, XGBoost may be more accurate than LightGBM.

In summary, if your dataset is very large and speed is an important factor, then LightGBM might be the best option. If accuracy is your
main concern, then you could opt for XGBoost. Either way, both tools are excellent options for decision tree-based machine learning
problems.



Chi-Squared Distribution:

	In probability theory and statistics, chi squared distribution with k degrees of freedom is the distribution of a sum of squares
of k independant standard normal random variables. it is a special case of gamma distribution and is used in constructing confidence
intervals and in hypothesis testing.

	the chi squared distribution is used in the common chi squared tests for goodness of fit of an observed distribution to a 
theoretical one. the independence of two criteria of classification of qualitative data.the independence of two criteria
of classification of qualitative data, and in confidence interval estimation for a population standard deviation of a normal distribution
from a sample standard deviation

	The X^2 test is carried out in the following steps:
	
	1. Subtract each expected number from each observed number. (O-E)
	2. Square the Difference.
	3. Divide the squares so obtained for each cell of the table by the
	   expected number for that cell.
	4. X^2 is the sum of (O-E)^2/E.


---------------------------------------------------------------------------------------------------------------------------------------
Contrastive Loss:

	Calculating Distance:  In 2 or 3 dimensions the euclidian distance (“ordinary” or straight-line distance) is a great choice for 
measuring the distance between two points. However, in a large dimensional space, all points tend to be far apart by the euclidian
measure. In higher dimensions, the angle between vectors is a more effective measure. The cosine distance measures the cosine of the 
angle between the vectors. The cosine of identical vectors is 1 while orthogonal and opposite vectors are 0 and -1 respectively.
More similar vectors will result in a larger number. Calculating the cosine distance is done by taking the dot product of the vectors.

	# P1 and p2 are nearly identically, thus close to 1.0
		pos_dot = p1.dot(p2)
		pos_dot -> 0.999999716600668
	# Most of the negatives are pretty far away, so small or negative
		num_neg = len(neg)
		neg_dot = np.zeros(num_neg)
		for i in range(num_neg):
			neg_dot[i] = p1.dot(neg[i])
		neg_dot -> [-0.91504053,  0.30543542, -0.37760565, -0.44443608, -0.64698801]


	Softmax: The softmax function takes a vector of real numbers and forces them into a range of 0 to 1 with the sum of 
all the numbers equaling 1. One other nice property of softmax is that one of the values is usually much bigger than the others. When 
calculating the loss for categorical cross-entropy, the first step is to take the softmax of the values, then the negative log of the 
labeled category.

Contrastive Loss:

	Contrastive loss looks suspiciously like the softmax function. That’s because it is, with the addition of the vector similarity 
and a temperature normalization factor. The similarity function is just the cosine distance that we talked about before. The other 
difference is that values in the denominator are the cosign distance from the positive example to the negative samples. Not very 
different from CrossEntropyLoss. The intuition here is that we want our similar vectors to be as close to 1 as possible, since
-log(1) = 0, that’s the optimal loss. We want the negative examples to be close to 0 , since any non-zero values will reduce the value
of similar vectors.

	# Contrastive loss of the example values
	# temp parameter
		t = 0.07
	# concatenated vector divided by the temp parameter
		logits = np.concatenate(([pos_dot], neg_dot))/t
	#e^x of the values
		exp = np.exp(logits)
	# we only need to take the log of the positive value over the sum of exp. 
		loss = - np.log(exp[0]/np.sum(exp))
		loss -> 4.9068650660314756e-05

Contrastive loss, like triplet and magnet loss, is used to map vectors that model the similarity of input items.

---------------------------------------------------------------------------------------------------------------------------------------

Skewness test:

If the skewness is between -0.5 and 0.5, the data are fairly symmetrical

· If the skewness is between -1 and — 0.5 or between 0.5 and 1, the data are moderately skewed

· If the skewness is less than -1 or greater than 1, the data are highly skewed



Kurtosis test:

 Kurtosis is one of the two measures that quantify shape of a distribution. kutosis determine the volume of the outlier

· Kurtosis describes the peakedness of the distribution.

· If the distribution is tall and thin it is called a leptokurtic distribution(Kurtosis > 3). Values in a leptokurtic distribution are near the mean or at the extremes.

· A flat distribution where the values are moderately spread out (i.e., unlike leptokurtic) is called platykurtic(Kurtosis <3) distribution.

· A distribution whose shape is in between a leptokurtic distribution and a platykurtic distribution is called a mesokurtic(Kurtosis=3) distribution. A mesokurtic distribution looks more close to a normal distribution.

Mesokurtic distributions

Mesokurtic distributions have kurtosis values close to that of a normal distribution. These distributions have a value of approximately 3 or an excess value near zero.
When sample data have kurtosis values that are notably different from the normal distribution, it indicates that the population might not follow a normal distribution.

Leptokurtic distributions – High Kurtosis

 These distributions have heavy tails that are longer and contain more extreme values. In short, there is a greater tendency for outliers. They have values of greater than 3 or positive excess values (> 0).

Platykurtic distributions – Low Kurtosis

Platykurtic distributions have less kurtosis than the normal distribution. They have lighter tails that are shorter and contain fewer outliers. These distributions have values of less than 3 
or negative excess values (< 0).


https://statisticsbyjim.com/basics/kurtosis/
https://github.com/manvendra7/Skewness-and-kurtosis

---------------------------------------------------------------------------------------------------------------------------------------

Time Series Analysis and Forecasting:


the difference between Multi and Univariate forecasting. The former uses only the previous values in time to forecast future values. The latter makes use 
of different predictors other than the series itself.

A useful abstraction for selecting forecasting methods is to break a time series down into systematic and unsystematic components.

Systematic: Components of the time series that have consistency or recurrence and can be described and modeled.
Non-Systematic: Components of the time series that cannot be directly modeled.
A given time series is thought to consist of three systematic components including level, trend, seasonality, and one non-systematic component called noise.

These components are defined as follows:

Level: The average value in the series.
Trend: The increasing or decreasing value in the series.
Seasonality: The repeating short-term cycle in the series.
Noise: The random variation in the series.

It is helpful to think of the components as combining either additively or multiplicatively.


An additive model suggests that the components are added together:
	y(t) = Level + Trend + Seasonality + Noise
	
	A linear trend is a straight line. A linear seasonality has the same frequency (width of cycles) and amplitude (height of cycles).
A additive model is linear where changes over time is consistently made by same amount.

A multiplicative model suggests that the components are multiplied together as follows:

	y(T) = level*trend*seasonality*Noise.

A multiplicative model is non linear like quadratic or exponential. Changes decrease or increase over time. A non-linear trend is a curved line.

A non-linear seasonality is a increasing or decreasing frequency and/or amplitude over time.

Decomposition as a tool:

	You may address it explicitly in terms of modeling the trend and subtracting it from your data, or implicitly by providing enough history for an algorithm 
to model a trend if it may exist.

	You may or may not be able to cleanly or perfectly break down your specific time series as an additive or multiplicative model. Real world problems are 
messy and noisy. There may be additive or multiplicative components. There may be an increasing or decreasing trend followed. There may be non-repeating cycles
mixed in with repeating seasonality components.

	
	The result object provides access to the trend and seasonal series as arrays. It also provides access to the residuals, 
which are the time series after the trend, and seasonal components are removed. Finally, the original or observed data is also stored.

	from statsmodels.tsa.seasonal import seasonal_decompose
	series = ...
	result = seasonal_decompose(series, model='additive')
	print(result.trend)
	print(result.seasonal)
	print(result.resid)
	print(result.observed)	
	
Multiplicative Decomposition

	Exponential changes can be made linear by data transforms. In this case, a quadratic trend can be made linear by taking the square root. An exponential growth
in seasonality may be made linear by taking the natural logarithm.

	Again, it is important to treat decomposition as a potentially useful analysis tool, but to consider exploring the many different ways it could be applied for
your problem, such as on data after it has been transformed or on residual model errors.
	

	
White noise:

White noise, by definition, is random data. A time series is white noise(random) if the variables are independent and identically distributed(i.i.d) with a mean 
of zero. In other words, the time series has a mean μ = 0 and a constant standard deviation σ = c. Since our mean is well above zero, we can rest assured 
that our data is not white noise.

Since the definition of white noise implies randomness, you cannot model it and make predictions. Once you see white noise,
do not continue with any predictive model!

Arima is short for Auto-Regressive Integrated Moving Average, which is a forecasting algorithm based on the assumption
that previous values carry inherent information and can be used to predict future values. time series that are univariate (one variable),
linear, and non-seasonal, which are great candidates for regular ARIMA models.

We can develop a predictive model to predict xt given past values., formally denoted as the following: p(xₜ | xₜ₋₁, … ,x₁)

The ARIMA model takes in three parameters:

1. p is the order of the AR term
2. q is the order of the MA term
3. d is the number of differencing

Autoregressive AR and Moving average MA:

a stationary time series has its statistical properties remain constant across time. So it is easier to model and make predictions based on such a series.
it has no clear trend, and oscillates around a mean of 0 with constant variance.

For different time series, we might be able to fit them with any of the combinations of the three components of ARIMA:

AR (Auto-Regressive): the time series is linearly regressed on its own past values
I (Integrated): if not stationary, the time series can be differenced to become stationary, i.e., 
compute the differences between consecutive observations
MA (Moving Average): the time series is ‘regressed’ on the past forecast errors

Here is the relationship of these three components in ARIMA: AR and MA are two models that usually work on stationary time series
– if the time series is not stationary, we can use the I part to make it stationary.

ARIMA parameters p, d, q:

	p: the number of past values included in the AR model
	
	d: the number of times the time series is differenced. For example, the first order differencing 
	
	q: the number of past forecast errors included in the MA model, or the size of the moving average window. 
	It’s named the MA model since each yt can be considered as a weighted moving average of the past forecast errors


Causation refers to a cause-and-effect relationship between two variables, where a change in one variable directly leads to a change in the other variable. 
It implies that the presence or absence of one variable causes the presence or absence of the other variable.Correlation, on the other hand, measures the 
statistical relationship between two variables, indicating how they tend to vary together. While correlation can help identify associations and patterns between
variables, it does not imply a causal relationship. Here are a few reasons why correlation does not imply causation:

Spurious correlation: Correlation can occur by chance or due to the influence of other variables that are not directly related. When two variables are correlated,
it does not necessarily mean that changes in one variable cause changes in the other. There may be an underlying third factor or a combination of factors driving
the observed correlation.

Reverse causality: Correlation does not provide information about the direction of causality. It is possible for two variables to be correlated, but the causal 
relationship may actually be the opposite of what is initially assumed.


These models rely on the assumption that the statistical properties of the time series, such as mean, variance, and autocorrelation, remain constant over time. 
If the data is non-stationary, the statistical properties may change over time, making it difficult to make accurate predictions.

A non-stationary time series may have a trend, meaning that the mean or the level of the series changes over time, or it may have a seasonality component,
where the patterns in the data repeat at regular intervals. In addition, a non-stationary time series may exhibit autocorrelation, where the correlation between
observations at different time points is not constant over time.

By transforming a non-stationary time series into a stationary time series, we can apply statistical models that assume stationarity, which can provide more
accurate predictions. Common techniques to transform non-stationary time series into stationary time series include differencing and detrending.	

ACF and PACF plots:

ACF is straightforward to measure. But in reality, the relationships among lags are more complicated. For example, we can assume that yt and yt-1 are correlated, 
and yt-1 and yt-2 are also correlated. Due to their correlations with yt-1, yt and yt-2 must also be correlated. How can we measure if there is new information
in yt-2 to predict yt, besides their relationships with yt-1?

That’s why we need another definition called partial autocorrelations. The PACF (partial autocorrelation function) shows the partial correlation of the time series
with its lags, after removing the effects of lower-order-lags between them. For example, the partial autocorrelation of yt and yt-k is the correlation that is not
explained by their relationships with the lags yt-1, yt-2, …, yt-k+1. It measures the balance amount of variance in yt-k to predict the future value of yt.


Plotting the partial autocorrelation function and drawing the lines of the confidence interval is a common way to analyze the order of an AR model. To evaluate the order,
one examines the plot to find the lag after which the partial autocorrelations are all within the confidence interval. This lag is determined to likely be the AR
model's order

Medium Article for ACF:
https://medium.com/@krzysztofdrelczuk/acf-autocorrelation-function-simple-explanation-with-python-example-492484c32711



Time Series Decomposition:

	Time Series data exhibit a variety of patterns and mainly shows three patterns: trend, seasonality and cycles. we usually combine trend and cycle into a single
component called as the trend-cycle component(sometimes called as the trend). thus the time series comprises of three components: a trend-cycle component, a seasonal
component, and a remainder component(anything else). 

		yt = St+ Tt+ Rt  (additive decomposition) or yt = St*Rt*Tt 
		
		where St = seasonal component, Tt = Trend cycle component, Rt = remainder component. 
		
    The additive decomposition is appropriate when the magnitude of seasonal fluctuations or variation around the trend cycle does not vary with the level of the 
time series.When the variation in the seasonal pattern, or the variation around the trend-cycle, appears to be proportional to the level of the time series, then
a multiplicative decomposition is more appropriate. Multiplicative decompositions are common with economic time series.

Seasonally Adjusted Data:

	If the seasonal component is removed from the original data, the resulting values are the "Seasonally adjusted data". yt-St gives for additive decomposition.
yt/St represents multiplicative seasonal decomposition. 

	If the variation due to seasonality is not of primary interest, the seasonally adjusted series can be useful. For example, monthly unemployment data are usually
seasonally adjusted in order to highlight variation due to the underlying state of the economy rather than the seasonal variation. An increase in unemployment due 
to school leavers seeking work is seasonal variation, while an increase in unemployment due to an economic recession is non-seasonal. Most economic analysts who 
study unemployment data are more interested in the non-seasonal variation. Consequently, employment data (and many other economic series) are usually seasonally 
adjusted.
	
	Seasonally adjusted series contain the remainder component as well as the trend-cycle. Therefore, they are not “smooth”, and “downturns” or “upturns” can be 
misleading. If the purpose is to look for turning points in a series, and interpret any changes in direction, then it is better to use the trend-cycle component 
rather than the seasonally adjusted data.


Moving Average(taking average over a window of length k):

The first step in a classical decomposition is to use a moving average method to estimate the trend-cycle. A moving average of order m is written as:

		Tt = 1/m * Summ(-k to k)yt+j.
		
where m = 2k + 1. that is the estimate of the trend-cycle at time t is obtained by averaging the values of the time series within k periods of time t. Observations
that are nearby in time are likely to be closer in value. the averaging eliminates the randomness in the data leaving a smooth trend-cycle component.which is called
as moving average of order m(m-MA).

The order of the moving average determines the smoothness of the trend-cycle estimate. In general, a larger order means a smoother curve.

Moving averages of moving averages:

	It is possible to apply a moving average to a moving average. One reason for doing this is to make an even-order moving average symmetric. For example, 
we might take a moving average of order 4, and then apply another moving average of order 2 to the results.

	When a 2-MA follows a moving average of an even order (such as 4), it is called a “centred moving average of order 4”.
This is because the results are now symmetric.It is now a weighted average of observations that is symmetric. In general, 
an even order MA should be followed by an even order MA to make it symmetric. Similarly, an odd order MA should be followed by an odd order MA.


Estimating the trend-cycle with seasonal data:

	The most common use of centred moving averages is for estimating the trend-cycle from seasonal data. Consider the  
2× 4 -MA.

	Tt = 1/8*(yt-2) + 1/4*(yt-1) + 1/4*(yt) + 1/4*(yt+1) + 1/8*(yt+2).
	
	When applied to quarterly data, each quarter of the year is given equal weight as the first and last terms apply to the same quarter in consecutive years.
Consequently, the seasonal variation will be averaged out and the resulting values of ^Tt will have little or no seasonal variation remaining. A similar effect
would be obtained using a  2×8 -MA or a  2×12 -MA to quarterly data.

	In general, a  2×m-MA is equivalent to a weighted moving average of order  m+1 where all observations take the weight  
1/m, except for the first and last terms which take weights  1/(2m). So, if the seasonal period is even and of order  m, we use a  2×m -MA to estimate the
trend-cycle. If the seasonal period is odd and of order  m, we use a  m-MA to estimate the trend-cycle. For example, a  2×12-MA can be used to estimate the 
trend-cycle of monthly data and a 7-MA can be used to estimate the trend-cycle of daily data with a weekly seasonality.

Other choices for the order of the MA will usually result in trend-cycle estimates being contaminated by the seasonality in the data.


Weighted moving averages

	Combinations of moving averages result in the weighted moving averages. For example the 2*4 MA is discussed above is equivalent to the weighted 5-MA with the 
weights given by [1/8,1/4,1/4,1/4,1/8]. In general, a weighted m-MA is written as:

		Tt = Summ{aj*y}
